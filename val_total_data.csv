index_article,index_paragraph,content,label
70,1,"After a decade of devolution and amid uncertainties about its effects, it is timely to assess and reflect upon the evidence and enduring meaning of any ‘economic dividend’ of devolution in the UK. Taking a multi-disciplinary approach utilising institutionalist and quantitative methods, this paper seeks to discern the nature and extent of any ‘economic dividend’ through a conceptual and empirical analysis of the relationships between spatial disparities, spatial economic policy and decentralisation. Situating the UK experience within the historical context of its evolving geographical political economy, we find: i) a varied and uneven nature of the relationships between regional disparities, spatial economic policy and decentralisation that change direction during specific time periods; ii) the role of national economic growth is pivotal in explaining spatial disparities and the nature and extent of their relationship with the particular forms of spatial economic policy and decentralisation deployed; and, iii) there is limited evidence that any ‘economic dividend’ of devolution has emerged but this remains difficult to discern because its likely effects are over-ridden by the role of national economic growth in decisively shaping the pattern of spatial disparities and in determining the scope and effects of spatial economic policy and decentralisation.",1
70,2,"The idea of an ‘economic dividend’ has gained significant momentum in the UK and internationally as a powerful and persuasive argument in the politics and economics of decentralisation; reflecting a broader international shift from identity to economy rationales in contemporary forms of state modernisation and ‘rescaling’ (Lobao et al. 2009; RodríguezPose and Sandall 2008). The ‘economic dividend’ is principally articulated in terms of generating allocative and productive efficiencies alongside the accountability and participation benefits of decentralisation for decision-making and co-ordinating collective action in support of economic development (Rodríguez-Pose and Gill 2005). In the UK in the late 1990s, for example, political advocates argued for devolution in Wales to improve its economic performance and productivity to create more and better jobs (Davies 1999). Regional government in the English regions too was promoted as an ‘economic imperative’ to address the ‘economic deficit’ of persistent spatial disparities between London, the Greater South East region and the rest (Murphy and Caborn 1996).",1
70,3,"More than a decade on from the main constitutional and devolutionary reforms in the UK, it is timely to assess and reflect upon the evidence and enduring meaning of the ‘economic dividend’ of devolution in the UK. Little research has been undertaken on this issue to date because it is a far from straightforward task, given the view that its impacts are “likely to be complex, subtle and difficult to measure” (Jeffery 2006: 1). Problems include: the development of appropriate proxies relevant to particular national contexts; assembling available data of appropriate quality, historical coverage and international comparability; disentangling and isolating the effects of decentralisation; and, attributing causation amongst decentralisation’s myriad relationships with broader economic and institutional change (Ashcroft et al. 2005; McGregor and Swales 2005).",1
70,4,"The approach taken here is multi-disciplinary, encompassing Economic and Political Geography, Local and Regional Development and Political Science. It utilises a geographically political economy that seeks to embed its institutionalist and quantitative analysis in its appropriate context of the unfolding histories of institutional, political and economic change over time and space within the particularity of the UK state. The argument is that analysis of any ‘economic dividend’ of devolution in the present needs to be rooted in the past evolutions of spatial disparities, spatial economic policy and decentralisation within the political-economies of particular nation states. The historical dimension of the analysis is critical in understanding the path dependencies that shape the evolution of institutional structures and policy approaches over time and space, reflecting legacies of political choices, strategies and struggles.",1
70,5,"With the aim of assessing and reflecting upon the evidence and enduring meaning of the ‘economic dividend’ of devolution in the UK over a decade after the institutional and political reforms introduced from 1997, the paper first addresses some conceptual issues in considering the relationships between spatial disparities, spatial economic policy and decentralisation. Second, the historical context and unfolding of the UK experience is outlined. Third, given the constraints of comparable data availability, the analysis focuses upon the period 1984-2007 in an attempt to discern the existence (or otherwise), extent and nature of any ‘economic dividend’ arising from the inter-relation of spatial disparities, spatial economic policy and devolution in the UK. Last, some conclusions and reflections are provided.",1
70,6,"Given its complex and unclear inter-relationships, one approach is to situate the consideration of any ‘economic dividend’ arising from devolution within an understanding of the changing relations between spatial disparities, spatial economic policy and decentralisation. An evolution is evident in the ways in which spatial disparities are understood and explained with implications for how spatial economic policy and decentralisation are formulated and unfolded. We discern stylised kinds of approaches – redistributive, free-market and growthoriented – with different characteristics concerning their economic theory, causal explanation of spatial disparities, adjustment process, policy rationales and instruments, institutional organisation, geographical focus and scope, political-economic project and language (Table 1) (see also OECD 2009). Spatial economic policy is seen as forms of economic policy with spatial intent – such as regional or urban policy. We recognise the need and difficulty of disentangling this from economic policy without explicit spatial intent but with spatial implications – such as macro-economic, welfare or defence policy. We acknowledge too the more recent debates about ‘spatially neutral’ or ‘blind’ policy that is focused upon ‘people’ rather than ‘place’ and explains spatial disparity as the compositional outcome of sorting processes driven by rational economic agents (see, for example, Overman 2010).",1
70,7,"Such transitions in spatial economic policy and its organisation reflect developments within economic theory and their differing causal explanations for spatial disparities and views of adjustment processes. Such conceptual and theoretical ideas are then mediated and translated into policy rationales and instruments within the institutional structures of particular national political economies (Pike and Tomaney 2009). Uneven, partial and messy transitions, overlaps and struggles undoubtedly mark the political economy of such shifts that play out in different ways in different national contexts. Indeed, part of the aim of this paper is to outline how this process has unfolded in the particular context of the UK.",1
70,8,"Integrally related to the changes in how spatial disparities are interpreted and spatial economic policy formulated is a marked shift in the geographical scale and level of institutional organisation, delivery and governance from centralisation toward varied forms of decentralisation. Indeed, it is critical to recognise that devolution is only one particular form of decentralisation and itself comes in different shapes and sizes, driven top-down and/or bottom-up by different levels of state and non-state actors, and with differing motivations and expectations (Rodríguez-Pose and Gill 2005; Torrisi et al. 2010). Table 2 outlines the main types which vary in their degree of autonomy in fiscal and functional terms, balance of reserved and decentralised powers and responsibilities, and administrative and/or democratic accountability. In emphasising the importance of historical context and evolution of national political economies, it is evident that as a ‘new state spatiality’ decentralisation will be highly variegated in different nation states (Peck and Theodore 2007).",1
70,9,"In considering whether and what any ‘economic dividend’ might mean, conceptualising the potential economic benefits and costs of decentralisation is central. Informed by the key arguments from fiscal federalism that focuses upon the vertical structure of the public sector and the existence of allocative and productive efficiencies and accountability and participation, Table 3 summarises some of the main issues. Several other concerns are also important to the analytical task. First, decentralisation is widely acknowledged as a process rather than a one-off event; there is little sense in splitting the analytical frame into ‘pre-’ and ‘post-’devolution periods. Indeed, understanding and capturing the timing and lag of any potential benefits and costs flowing from decentralisation has bedevilled studies to date (McGregor and Swales 2005). Second, it is important to distinguish the expected or likely benefits and costs of specific forms of decentralisation such as devolution from what are considered the unexpected, unlikely and/or somehow additional bonus or windfall economic implications implied by the term ‘dividend’. Last, given that much of the discussion of the ‘economic dividend’ to date has focused upon potential efficiency benefits, attention is required to the equity concerns of the distributional impacts of decentralisation in economic, social and geographical terms.",1
70,10,"While the aspiration might be to produce some kind of assessment of the overall net balance of economic outcomes generated from both positive and negative effects of devolution, the inter-relatedness of spatial disparities, spatial economic policy and decentralisation involved means a more finely grained and nuanced account is required. Analysts searching for the ‘economic dividend’ need to accept that it may or may not exist and that its extent and nature over time and space are likely to be highly variable and differentiated; reflecting potentially strong, weak and indifferent degrees of both positive and negative impacts as well as place-specific spill-overs from the economic into different kinds of political, social and cultural domains in the political economies of particular nation states and territories.",1
70,11,"Entrenched and persistent spatial disparities have marked the UK, especially since the 1930s, and continue to exert an enduring influence upon the national political economy, politics and policy (Figure 1). Significantly, traditional interpretations of spatial disparities emphasised the economic inefficiency (rather than benefit or dividend) of the geographic overconcentration and centralisation of economic activities in London and the Greater South East (Martin 2008), hampering national economic growth because of the rapid generation of inflationary bottlenecks in factor markets during periods of expansion that were stymied by macro-economic policy before their benefits could trickle down to the peripheral regions (Armstrong and Taylor 2000). Informed by the new economic geography (e.g. Krugman, 1991), more recent explanatory narratives emphasise the economic benefits and even dividends of spatial disparities in the UK based upon the powerful growth enhancing benefits of localised agglomeration in London and the Greater South East. This analysis argues that any spatial policy should aim to correct market failures in support of this geographic concentration because dilution or redistribution of its effects would be detrimental to overall growth and welfare (see, for example, Leunig and Swaffield 2008).",1
70,12,"The UK’s experience of persistent spatial disparities has configured a long history of spatial economic policy focused upon the regional scale, punctuated by ‘policy on’ and policy off’ episodes alongside ongoing, periodic scrutiny and reflection upon its principles and purpose (see, for example, Barlow 1940; Harrison and Hart 1993; House of Commons 1995; 2003). Inter-war efforts focused upon addressing localised concentrations of high unemployment and poverty generated by industrial decline in South Wales, northern England and west central Scotland (Martin 1988). After 1945, post-war consensus and persistent cross-party support for regional policy was predicated on traditional efficiency and equity rationales as well as the desire for class-based national parties to avoid the emergence of separate territorial politics (Gordon 1990). ‘Spatial Keynesianism’ (Martin and Sunley 1997) marked state intervention through regional policy, focused upon investment and stimulation of cumulative causation and the management of aggregate demand and employment in the regions (Kaldor 1970).",1
70,13,"Despite some success in job creation and economic diversification in the Assisted Areas (Taylor and Wren 1997), the political-economic tide turned away from Keynesianism and its redistributive spatial policy toward neo-liberalism following the crisis of stagflation, industrial strife and public fiscal imbalances during the 1970s. Characterised by deregulation, liberalisation and the attempted ‘rolling-back’ of the state, the UK variant led by Margaret Thatcher’s Conservative administrations from 1979 emphasised individual responsibility, free markets and enterprise which underpinned the critique and dismantling of regional policy during the 1980s. The turn toward neo-classical economics and the free market interpreted Keynesianism and regional policy as distortions and impediments to rational and efficient decision-making amongst economic actors. Subsidies were seen as economically inefficient and wasteful, causing ‘deadweight’ effects in supporting activities that would have occurred anyway and unable to tackle structural problems including lack of enterprise and innovation (Wren 2005).",1
70,14,"In contrast to the Keynesian emphasis upon demand, neo-classical theory emphasised intervention to enhance the flexibility and upgrading of the supply-side of factor markets such as labour skills. Structural change toward services in the UK economy favoured regions and localities around London and the Greater South East, sharpening the North-South Divide in spatial disparities (Martin 1988). Spatial economic policy under Thatcherism reduced spending, shrunk the map of eligible Assisted Areas, changed support from automatic to discretionary selectively to encourage small enterprise and the attraction of international inward investment flows in the context of the Single European Market (see Figure 2). Formerly state-owned industries such as coal, steel and shipbuilding were privatised and rationalised with highly damaging localised impacts (Hudson 1989). The geographical focus of spatial policy shifted to the urban and the institutional lead and resources were transferred from local government to new special purpose bodies such as Urban Development Corporations and Local Enterprise Agencies (Martin and Tyler 1992).",1
70,15,"The UK’s long history as a highly centralised, multi-national ‘union state’ has shaped the evolution of its institutional structures in formulating and delivering spatial economic policy with only limited and conditional decentralisation. Variegation is evident in the UK state’s institutional forms, powers and resources within its nations and regions through cumulative administrative decentralisation to the Scottish Office (from 1885), Wales Office (1964), Stormont Parliament (1921) and Government Offices in the English regions (1994). Despite the short-lived experiment with Regional Planning Councils in England during the 1960s and the establishment of the Welsh Development Agency (WDA) and Scottish Development Agency (SDA) in the mid-1970s, centralisation has marked the institutional arrangements for addressing spatial disparities through spatial economic policy.",1
70,16,"Following the programme of devolution and constitutional change from 1997, the geographically differentiated institutional and political legacy was effectively built upon by further but highly uneven democratic decentralisation, creating a multi-level, polycentric UK state working across several geographical scales and representing a more distributed landscape of political power (Morgan 2007). Despite such reforms, the UK remains highly centralised in comparison to other western European states (Marks et al. 2008). The new arrangements comprised an asymmetrical hierarchy of powers and resources ranging from the Parliament in Scotland, through the National Assemblies in Wales and Northern Ireland, to the Mayoralty and Assembly in London and, until their abolition in 2010, the Regional Development Agencies and indirectly elected Regional Chambers in the English regions (Tomaney 2000). Territorial politics have subsequently been reinvigorated and become evident in varying degrees of differentiation in spatial economic policy and varieties of institutional organisation divergent from national, largely English-regions focused frameworks (Adams et al. 2003).",1
70,17,"England’s size and weight within the UK – constituting some 80% of the total population and nearly 90% of GDP – renders considerations of governing its uneven development and growth important and difficult for the UK state (Morgan 2001). The political and institutional settlement for the English regions remained ‘unfinished business’ (Tomaney 2000). Given a central economic leadership role, the RDAs were established in 1998 as “an essential first step to provide for effective, properly co-ordinated regional economic development... and to enable the English regions to improve their competitiveness” (Department of Environment 1997: 1), accountable nationally through their sponsoring Department and Minister and regionally through indirectly elected Regional Chambers.",1
70,18,"The widely touted and anticipated ‘economic dividend’ predicated upon heightened autonomy, resources and political voice at the national centre was seen as a potential route to a step-change in economic performance capable of closing the prosperity gap with London and the Greater South East on a sustainable basis. Echoing the concepts we outlined earlier, the potential ‘economic dividend’ attached to such devolution was to be delivered through enhanced autonomy to design and implement policies tailored to regional and local needs (allocative efficiencies) and more responsive, effective and accountable governance systems providing decentralised institutional capacity to mobilise and shape collective action for developmental ends (productive efficiencies). Amidst its weak powers, the uneven enthusiasm as well as hostility in central government, the lack of faith in national government late in its second term and the broader currents of distrust in politicians and politic institutions, the particular form of ERAs proposed were rejected 3:1 in the sole referendum in North East England in late 2004 (Rallings and Thrasher 2005).",1
70,19,"As the territorial politics of devolution shaped the particular developments in Northern Ireland, Scotland and Wales, the vacuum left by the faltering regionalism and regionalisation projects in the English regions was filled by a range of emergent spatial imaginaries claiming to offer an institutional fix for the persistent problem of governing spatially uneven development: resurgent cities and/or city-regions as motors of their regional economies enhancing their economic performance; localism led by local authorities capable of decentralised approaches to economic development; and pan-regionalisms focused upon cross-regional issues including housing, jobs and infrastructure in newly designated ‘Growth Areas’, housing market renewal areas in northern cities and the Northern Way linking RDA activity to close the productivity gap with London and the Greater South East across the three northern regions (Pike and Tomaney 2009). Following the election of the coalition government in 2010, debate has raged about the shift in focus toward ‘decentralisation’ and ‘localism’ and the dismantling of the regional tier through the winding-up of the RDAs and the abolition of Government Offices for the Regions.",1
70,20,"That ‘devolution is a process not an event’ (Davies 1999) in the words of the former Secretary of State for Wales has become a well worn descriptor that nonetheless characterizes the evolutionary and unfolding nature of decentralisation. Hence, while the analysis here is focused upon discerning the existence or otherwise of a ‘economic dividend’ in the UK following the substantive institutional changes introduced through devolution and constitutional change from 1997, the argument is that such changes need to be situated in the context of relationships between spatial disparities, spatial economic policy and decentralisation and reflect deep seated currents and legacies from previous eras of change. Our approach is based on the combined historical paths of three key elements – spatial disparities, spatial economic policy and decentralisation – during a period ranging from 1984 to 2007 (local revenue data are only available for a shorter period from 1987). Building upon Rodríguez-Pose and Gill (2004), we simultaneously consider the historical evolution of spatial disparities and decentralisation but introduce a new dimension by augmenting the analysis with a quantitative measure of spatial economic policy.",1
70,21,"More than a decade after the constitutional reforms of the late 1990s which brought devolution to parts of the UK, this article has sought to assess and reflect upon the evidence and enduring meaning of the ‘economic dividend’ of devolution in the UK. Acknowledging the difficulties and methodological challenges involved in seeking to discern such a complex, subtle and difficult to measure entity (Jeffery 2006), a multi-disciplinary and geographical political economy approach has been adopted. This study has sought to embed any relationships between decentralisation, spatial economic policy and changes in spatial disparities in their appropriate context of the unfolding histories of institutional, political and economic change over time and space within the particularity of the UK state. The UK’s highly centralised system has marked its particular evolution and established a constrained context for any ‘economic dividend’ to emerge. This contrasts the more substantive and extensive fiscal decentralisation in other countries which have experienced stronger, more widespread, positive effects under certain conditions (Rodríguez-Pose and Ezcurra 2010).",1
70,22,"The analysis has revealed a number of interesting points. First is the varied and uneven nature of the relationship between spatial disparities, spatial economic policy and fiscal devolution in the UK, with important changes in direction during the period under analysis. Second, the role of national economic growth is pivotal in explaining the evolution of spatial disparities and the nature and extent of their relationship with the particular forms of spatial economic policy and decentralisation deployed. The dominance of national economic growth within the current forms of more growth-oriented forms of spatial economic policy has reinforced its decisive explanatory role, underlining the emergent tradeoff between national economic growth and politically tolerable levels of spatial disparities (Martin 2008).",1
70,23,"Our findings lead us to conclude that even when it might be discerned, any ‘economic dividend’ of devolution is likely to be highly variable, taking different forms and degrees, and may be episodic or fleeting in its duration. It appears highly contingent upon particular paths of state institutional change across a range of scales and to be strongly shaped by national economic growth, the nature of fiscal autonomy and capacity and willingness for redistribution on the part of national central states. We acknowledge, however, that it might be that the timescale of our assessment may be too foreshortened and that much more than a decade needs to elapse before the effects of any ‘economic dividend’ become more apparent. In addition, improvements are needed in data availability and methodological development to help create further proxies and indicators.",1
70,24,"Amidst the territorial politics of the UK’s nations and regions, contestation over the existence, nature and scale of any ‘dividends’ associated with devolution continue to unfold, further stoked by the emergence of a Conservative-Liberal Democrat coalition government in 2010. The uneven ways in which devolution has been unfurled across the UK’s polycentric political economy continues to generate political controversy. In 2010, for example, a Scotland Office review found that 1999 public expenditure in Scotland by both the central and devolved administration exceeded the total revenue raised from across Scotland by ￡75.8bn. This led the then Scottish Secretary Jim Murphy to claim that “Scotland has two governments spending billions of pounds of public money and there is a clear and quantifiable ‘devolution dividend’. Scotland gets the best of both worlds from devolution” amidst protests from the Scottish National Party that the figures were inaccurate. The continued relevance of this concern with the economic and indeed other dividends of devolution warrants further studies, especially international comparative work.",1
71,1,"This paper describes an approach developed to measure regional economic resilience across Europe which is novel in three key dimensions. Firstly, it seeks to date regional downturns as opposed to assuming that all regional economies are affected by economic shocks at the same point in time; secondly, it measures the amplitude and duration of economic downturns and subsequent recoveries; and thirdly, as well as measuring recovery, it measures the resistance of regional economies to economic shocks. The paper applies this methodology to selected European countries to provide an analysis of differential regional responses to several economic shocks since the early 1990s. The paper then reflects upon the utility of this methodology for operationalising regional economic resilience in crosscomparative studies.",1
71,2,"The effects of the post-2008 economic crisis across Europe have been widespread and significantly contagious but also highly geographically uneven, thereby drawing attention to differences between regions in their vulnerability to economic shocks and their ability to adapt to serious disruptions in the economic environment (Jones et al, 2010; Martin, 2011; Hadjimichalis and Hudson, 2014). This has intensified the interest of evolutionary economic geographers in the concept of regional economic resilience which offers the potential to illuminate the capacities of local and regional economies to withstand and recover from economic shocks, and to adapt their development paths accordingly (Davies, 2011; Fingleton et al, 2012; Martin, 2012). Given the breadth of the disciplinary origins and applications of the resilience concept however, coupled with the relative newness of evolutionary theorizing in economic geography, the conceptual framing and clarity of the notion of regional economic resilience remains the subject of considerable academic debate (Bristow and Healy, 2014; Martin and Sunley, 2014; Boschma, 2014).",1
71,3,"Nothwithstanding this, economic resilience has quickly gathered credence as a concept with policy-makers and practitioners seeking to understand both why some places are better able than others to withstand economic shocks and/or recover quickly from them, and what they themselves might do to influence these capacities (Dawley et al, 2010; CLES, 2010). Indeed, policy discourse around economic development at national, regional and local scales is increasingly replete with talk of the importance of ‘building a resilient economy’ (Osborne, 2014).",1
71,4,"Recent years have witnessed considerable growth in the number and variety of resilience indicators and toolkits being used by practitioners eager to understand whether a particular local or regional economy is resilient, and policy-makers keen to assess and benchmark the resilience of their economies relative to others (see, for example, CLES, 2010; Greenham et al, 2013; IPPR North, 2014; ARUP, 2014). However, the practical development of indicators has clearly run ahead of conceptual thinking on resilience. There is indeed no single agreed approach to the measurement of resilience and the growing diversity of indicators risks further diluting the clarity and utility of the resilience concept (Christopherson et al, 2010; Martin and Sunley, 2014). Existing indices remain largely unproven and past indices have proved to be inaccurate in predicting the resilience of economies to the most recent economic crisis (Briguglio et al, 2006). Yet robust measures of resilience are clearly needed, not least to help illuminate what Rose and Krausmann (2013) refer to as ‘actionable variables’, or the key elements that can be influenced by regional actors in processes of shock recovery. In our rush to identify what makes an economy resilient to economic shocks we are in danger of losing an objective means of first identifying which economies were resilient to an economic shock and which were not.",1
71,5,"The purpose of this paper is to contribute to this debate by developing an approach for operationalising the concept of regional economic resilience in a cross-comparative analysis of the effects of the post-2008 global financial crisis on European regions. We develop an approach which focuses on measuring resilience in terms of post-shock outcomes, but which adapts available methods for dating regional business cycles to capture differences in both the timing of when the shock hit regions, and the amplitude and duration of both the downturns experienced and subsequent recoveries. The paper is now structured as follows. The next section examines the challenges which surround operationalising the complex, evolutionary conception of regional economic resilience. In section three, we detail the distinctive features of our approach, whilst section four illustrates some key results from applying this approach to the regional experiences of the recent economic crisis across Europe. Section five provides reflections on the utility of this approach and the paper concludes by identifying how this may help advance the operationalisation of resilience in cross-comparative research.",1
71,6,"Evolutionary theorists also assert that resilience must be conceived as a multi-dimensional and indeed processual entity. This embraces the need to understand not only the nature and duration of the shock and the region’s vulnerability to it, but also its capacities to withstand or resist the shock in the first place, the robustness of its firms and institutions in responding to it, and the extent and nature of the regional economy’s recovery from it (Martin 2012; Martin and Sunley, 2014). This complexity is the source of one of the critical problems which has surrounded much of the resilience literature particularly in relation to its measurement, and that is the tendency to conflate and confuse resilience outcomes and resilience capacities (Bristow and Healy, 2014).",1
71,7,"This implies that in measuring resilience, it is critically important to distinguish between the measurement of a region’s specific post-shock outcomes, or its revealed resilience, and measurement of the region’s resilience capacities. Indicators of adaptive capacity, many of which feature in some of the developing policy and practice metrics of resilience referred to earlier, do not reveal resilience. They simply point to resilience capacities or the adaptive mechanisms and processes which imbue a regional economy with the means to be resilient (as Martin and Sunley, 2014). They are the factors that need to be examined in order to understand how and why resilience outcomes vary. These factors may be shaped by a wide range of structural and behavioural factors and attributes, the relative importance of which is subject to increasing debate and which is likely to require both quantitative assessment and a more localized and qualitative ontology (Bristow and Healy, 2014; Martin and Sunley, 2014).",1
71,8,"The appropriate metric for identifying this reference state also needs to be considered. Measuring resilience as performance or outcome also requires identification of comparable economic indicators (such as GDP and employment), an understanding of how to deal with mixed results and trade-offs (e.g. sometimes output will show positive performance whereas employment will not), as well as an understanding of what the perceived resistance or recovery state is in the absence of a known equilibrium. This is particularly challenging when it is apparent that shocks may have a diverse range of hysteretic effects on the development paths of regions, permanently changing the composition of their economies or the behaviour of key actors within them (Martin, 2012). A related challenge is to determine whether resilience is being analysed in absolute terms (how all regions performed relative to the shock) or in relative terms (which regions performed better or were more resilient than others). Resilient outcomes may be judged relative to a region’s own reference state or ‘norms’ (pre-shock levels, patterns and fluctuations in these performance indicators). They may also be judged in comparative context (with other regions in the same nation or in other nations).",1
71,9,"A key challenge for any study examining economic resilience is identifying when a shock has actually occurred. Taking the argument that an economy is always reacting to changing circumstances, that it is never actually in a state of ‘equilibrium’ (Martin, 2012), means that it is always beset by economic shocks, some minor and some major. It is only when these are of a certain magnitude, or occur in a particular context, that the effects become observable. In this regard, economic shocks can be compared to physical earthquakes. Small regular tremors pass without observation except by the most sensitive instruments. More sizeable earthquakes cause greater levels of damage, depending on the extent to which a place had planned and prepared for such an event.",1
71,10,"In this complex setting, identifying the onset of a particular system-wide shock for comparative analysis can be challenging. Whilst it may be tempting to look for a particular incident from which to date a shock, this can serve to confuse cause and effect. For example, although the collapse of the US firm Lehmann Brothers is often cited as a key point in the recent financial crisis, the roots of this lay in the sub-prime mortgage crisis following the collapse of the US housing bubble (Gamble, 2009). The effects of this reverberated around much of the world through a series of complex interactions (Martin, 2011; Hadjimichaelis and Hudson, 2014), coupled with second and third order effects channelled through financial markets, trade links, and behavioural changes, whereby citizens amended their consumption and savings behaviour on the basis of their wider expectations of the future (Hadjimichaelis, 2011; Smith, 2013; Hannon, 2014).",1
71,11,"A further challenge concerns the time period given for resilience outcomes to be revealed. Existing literature suggests that this is very much a matter of judgement. In a study of regional economic resilience in the US, Hill et al (2011) consider a region to be resilient if (as a minimum) it returns to its prior growth path within a relatively short period of time, namely within four years. More generally, questions surround the relationship between short-term resilience to shocks and a region’s long-run regional development pattern. Martin and Sunley (2014) argue that short-term measures of resilience as ‘bounce-back’ to some pre-shock state or norm, have to be understood as constitutive of long-term regional growth paths and development trajectories. As such, understanding short-term resilience outcomes may be critical to understanding longer-term patterns of regional convergence and divergence.",1
71,12,"A final challenge is the scale of analysis. Much analysis is undertaken at the national level, but there is increasing interest in the economic resilience of sub-national units, such as regions (Fingleton et al, 2012), cities (Capello et al, 2015), or other administrative or statistical units (Doran and Fingleton, 2014). Resilience to economic shocks may vary by scale however: national economic resilience may not necessarily mean constituent regions and localities, with their diverse characteristics and development paths, will necessarily exhibit resilience to the same shock (Martin, 2012).",1
71,13,"One of the classic conundrums for studies of resilience is how that resilience should be measured. Whilst some writers have taken the approach of developing a basket of indicators (Briguglio et al 2006; CLES, 2010), this may conflate cause and effect. In order to understand what might make a region resilient to economic shocks we need to be able to measure its resilience in a way that does not lead to later problems of autocorrelation.",1
71,14,"We have chosen to use two data series; firstly the level of employment in a region and, secondly, the level of Gross Domestic Product (GDP). Pragmatically, both are consistently available on a comparative basis across the EU territory. Employment is a more meaningful measure than GDP as it counts the number of people employed in a region and is less prone to revision (see Coyle, 2014). In addition, it has a social value as there is a tendency in the minds of the public and politicians to regard the possession of a job as a strong indication of the well-being of an economy. However, as GDP is a standard measure of economic wellbeing and tends to be used to measure entry and exit from recession, we also consider this. We use real (in 2005 constant prices) GDP data since this offers a better perspective when tracking income and output resilience over a period of time.",1
71,15,"To understand how resilient regions are, we first need to date their business cycle turning points. We can then calculate the amount of GDP or employment loss between the peak and trough turning points of the cycle. In our approach we treat each region as a separate time series and then date the individual business cycle turning points. This allows us gauge resilience by measuring how much output or employment is lost over downturns, and to calculate the time to recovery. This approach builds on Sensier and Artis (2014) which dates countries within the UK employment cycles, and adds flexibility to the approach of Martin (2012) which assumes that all regions have the same turning point dates as the national employment series.",1
71,16,"For the purposes of our work, we have defined regions as synonymous with the NUTS 2 regional classification of Eurostat. Although there are conceptual challenges with this approach (McLeod, 2001), the availability of consistent datasets at this scale outweighs these difficulties. For comparative purposes we also make use of data at the NUTS 3 territorial scale, which we refer to as ‘local’ in order to differentiate from the regional scale.",1
72,1,"Economic growth is one of the most widely studied issues in the specialised literature. Economic growth highlights those changes that occur in enhancing macroeconomic results that are not expressed independently, but in close connection with its determinants. This article aims at modelling the economic growth of the member states of the European Union in relation to the economic freedom index and the index of economic progress.",1
72,2,"We consider the following objectives: the study of the intensity of relationships between GDP per capita and index of economic freedom on one hand, and social progress index on the other; modelling growth relative index of economic freedom and social progress index for the EU Member States Union. To investigate these topics, we have used descriptive analysis, analysis of variance, correlation analysis, and multiple regression analysis.",1
72,3,"We have developed a multiple regression model to study the influence that the index of economic freedom and of the social progress index have on the growth rate of GDP per capita in countries which have the status of member the European Union. Through variance analysis, we have concluded that there are significant differences among the average GDP per capita in the European countries according the membership of the European Union.",1
72,4,"In liberal tradition, since Adam Smith, economic freedom has proven to be the best path to prosperity. It is the key issue for all countries, but in order to consider it a progress it has to be accompanied by a general social and spiritual progress. Economic growth stands for the real increase, in a certain period of time and within a certain space, of an aggregate economic indicator such as GDP.",1
72,5,"Economic growth highlights those changes occurring in the increase of the macroeconomic results expressed not independently, but in close connection with its determinants. A wide range of both theoretical and empirical studies points out to a great variety of determinants of this process: investment, accumulation of physical capital, human capital, innovation, geography, political and legal institutions, macroeconomic conditions etc. are all considered to play a role in enhancing economic performance (Bassanini and Scarpetta, 2001; Petrakos and Arvanitidis, 2008; Moral-Benito, 2009).",1
72,6,"There have been many attempts to quantify the degree of economic freedom. A heterogeneous range of indicators which aim at quantifying economic freedom has been developed (for example, Economic Freedom of the World from Fraser Institute, Index of Economic Freedom released by Heritage Foundation & Wall Street Journal, World Survey of Economic Freedom from Freedom House). They share several similarities since most of them include in their methodology the core elements of economic freedom: secure private property rights, rule of law, freedom of trade, limited government (Iacobu?? and Gagea, 2010).",1
72,7,"In this paper economic freedom is analysed using the Index of Economic Freedom from Heritage Foundation & Wall Street Journal. It is an aggregate indicator, determined as the mean of the scores for each of its 10 components. The components of the indicator are grouped into 4 categories, namely Rule of law, Limited government, Regulatory efficiency and Open markets (Heritage Foundation, 2016). According to the index methodology, for each of the 10 components, a country receives a score which ranges from 0 to 100. The higher the score is, the higher the degree of freedom included in that component.",1
72,8,"The database includes variables concerning 40 European countries. We considered the following variables: the European Region the state is a part of ( “Eastern Europe”, “Western Europe”, “Northern Europe” , and “South of Europe”), membership of the European Union (“Yes”, “No”), GDP per inhabitant (2014), GDP per inhabitant (2013), Index of Economic Freedom, Index of Social Progress, and rate of growth of GDP per inhabitant. The sources of data are WorldBank, Heritage Organization, and Social Progress Imperative.",1
72,9,"Variance analysis is a statistical method that can be used to analyze the variable variation in relation to influencing factors. ANOVA consists of total variance decomposition of a set of data in source components variance (causes) and comparing them to determine whether those factors were significantly influencedthe variable. In order to to see if variable Membership of the European Union affects GDP per capita we used ANOVA. Correlation analysis was used for the study of the intensity of the relationship between the economic freedom index and GDP per capita, and between economic freedom index and index of social progress. With the aim of modelling the economic growth on the base on economic freedom index and index of social progress for European countries a multiple regression model was used. This method shows the intensity of the relationship between the factors being considered and their influence on the studied phenomenon. To validate the econometric model, we verified the fulfillment of assumptions both in the deterministic component and in the stochastic component. Stochastic component checks the following assumptions: the influence of the error term in the series is null, the hypothesis of homoscedasticity, the hypothesis of independence, and the hypothesis of normality.",1
72,10,"Economic growth is a complex process targeting the economic system as a whole and in its dynamics. Economic growth highlights those changes occurring in the increase of the macroeconomic results expressed not independently, but in close connection with its determinants.",1
72,11,"This study aimed at analysing economic growth in relation to the index of economic freedom and social progress index. After a preliminary analysis of the distributions of variables for the 40 countries considered in our sample, we conclude that the Southern and Eastern Europe countries present lower scores compared to the countries in other regions. Thus, many of the Southern and Eastern states have scores lower than 63 for the index of economic freedom and lower than 78 in terms of social progress index.",1
72,12,"In terms of the direction of GDP growth rate, we aimed at testing if it is influenced by the values of the two indices considered in the analysis. To determine this, we estimated a logistic regression model. However, the variables included in the model were not statistically significant. Thus, the two indices do not influence the direction of the GDP per capita growth rate, this one being influenced by other factors not included in our model.",1
73,1,This paper introduces Economic Space notion to expand capacity for economic and financial modeling. Introduction of Economic Space allows defining economic variables as functions of time and coordinates and opens the way for treating economic and financial relations similar to mathematical physics equations. Economic Space allows study of economic models on discreet and continuous spaces with different dimensions. The number of risks measured simultaneously determines Economic Space dimension. We present examples of modeling on Economic Space: option pricing and derivation of Black–Scholes–Merton equation on n-dimensional Economic Space; Markov processes and derivation of Fokker–Plank Equations. Usage of Economic Space allows construing approximations of Economics and Finance similar to physical kinetics and hydrodynamics and derives Wave Equations for Economic and Financial variables.,1
73,2,"Economics and finance are systems with extreme complexity similar to complexity of theoretical physics; nevertheless the phenomena of these disciplines are too different. During the last decades Econophysics has delivered many contributions for understanding and modeling economic and financial features (Mantegna & Stanley, 2000; Roehner, 2002; Stanley, 2003; McCauley, 2006; Special Issue, 2008; Schinckus, 2013) and presented many applications of statistical physics methods to economics and finance. Methods and models developed by theoretical and statistical physics might be useful for description of economics and finance. These results present first steps for modeling economic processes with accuracy similar to and with understanding of comparable to current description of physical processes. We treat Econophysics as a way to adopt current methods and schemes of theoretical and statistical physics to economic and financial modeling. Such adoption should follow economic and financial laws and these requirements change foundation basement of most methods and models developed within theoretical and statistical physics.",1
73,3,We study economic analogies of physical schemes that can be useful for economic and financial modeling. One of the most general and common physical notions is the space–time issue. We state a simple question: is it possible to introduce certain analogy of space–time notion for economic and financial modeling and what are the possible advantages? Further we mention such analogy as Economic Space.,1
73,4,"We introduce Economic Space notion as an extension of matter that is already used in economics for many years and thus we assume that Economic Space notion conforms to economic phenomenology. At the same time Economic Space notion allows regarding economic variables as functions of time and Economic Space coordinates. It allows treating economic and financial properties in a way similar to mathematical physics equations and applies methods of current theoretical physics to economic modeling. It is already well known (McCauley, 2006)and we also underline the statement that the lack of conservation laws and symmetries in economics and finance makes differences between economic and physical systems vital. Theoretical physics methods should be changed to obey economic and financial phenomenology. Space–time determines the foundation of theoretical physics and description of Nature. Thus motivation of our study is based on assumption that introduction of Economic Space may give a new look on Economics and build solid foundation for broad usage of theoretical physics methods for economic and financial modeling.",1
73,5,"We introduce Economic Space notion with the goal to study economic variables of economic agents and economic variables of entire macroeconomics as functions of time and Economic Space coordinates. Economic variables can describe Supply and Demand, Loans and Debts, Labor and Taxis, Production Function and Capital and so on. Our definition of Economic Space notion has nothing in common with spatial econometrics (Anselin, 2009) and our approach completely differs from agent-based economics (Judd & Tesfatsion, 2005). We assume that different economic states allow define different “intrinsic” Economic Spaces that describe different approximations of economic and financial processes. Existence of such Economic Space and opportunity describes economic variables as functions of time and Economic Space coordinates allows use functional analysis, stochastic functions and modern methods of mathematical physics for economic and financial modeling.",1
73,6,Introduction of Economic Space for economic modeling arises two problems. First: how to define Economic Space? Second: how to measure coordinates of economic variables and how to define the partition of economic variables over certain Economic Space? To solve these problems we suggest use methods that are well known in economics for many years. We refer to Lee (1999); BIS (2011) and BIS (2013) as a small amount of economic and financial risk management studies. Risk management deals mostly with risk ratings of banks and corporations. Risk ratings are provided by international rating agencies like Fitch (2006); Moody's's (2007); S&P (2012) and DBRS (2013). We state that Economic Space notion and economic and financial modeling on Economic Space can be developed on the basis of risk rating practice,1
73,7,"To show that let outline that risk ratings procedures distribute economic agents as companies, corporations, and banks over a finite number of risk grades, like AAA, BB, CCC and so on. Finite number of risk grades can be treated as finite number of points of discreet space. Risk grades of economic agents like banks or corporations can be treated as their coordinates on discreet space. Risk ratings estimations of economic agents are similar to measurements of coordinates of economic agents on discreet space. Ratings of single risk can be treated as coordinates on one-dimensional discreet space. Simultaneous estimations of ratings of n risks of economic agents are similar to measuring coordinates on n-dimensional discreet space. Thus existing risk ratings practices can be treated as procedures that distribute economic agents on discreet space. We suggest mention such space as Economic Space. Let associate risk ratings or coordinates of particular economic agent with coordinates of its economic variables. That defines economic variables of economic agent as functions on Economic Space.",1
73,8,Our treatments of option pricing theory on n-dimensional e-space Rn allow derive extension for the BSM equation and uncover additional difficulties of option pricing modeling. These problems concern requirement to take into account possible changes of main risks that determines current e-space representation. Random dynamics of main risks during time to expiration means that options pricing Eq. (6) should be transformed into other ones on e-space with different axis. That effect might explain variances between predicted and observed option price dynamics. Correct description of these effects might rise up accuracy of option pricing.,1
73,9,"E-space notion uncovers extreme internal complexity of financial and economic systems. Main difficulties concern econometric problems and observation, choice of most valuable risks and measurements of e-particles distributions on e-space. At present, there are no risk ratings methodologies that allow distribute economic agents on Rn. Development of econometric and economic statistics that may establish empirical ground for e-space modeling is the most crucial problem. Cooperative efforts of Rating Agencies and Businesses, Government Statistical Bureaus and Research Communities, Banks and Regulators, etc., are required to establish distributions of economic agents on e-space with one or two dimensions. It is necessary to unify risk ratings methodologies, develop methods to measure and compare different risks, methods to chose most valuable risks and so on. We propose that even simplest model of e-space distributions of macroeconomic variables can allow observe dynamics of economic variables, visualize macroeconomic distributions like Supply and Demand, describe e-space states of Inflation, Financial Markets, Derivatives Markets and more.",1
70,1,"It is timely to evaluate the evidence for any economic benefit of devolution in the UK after ten years of implementation and amidst uncertainties regarding its effects. To achieve this, a multi-disciplinary approach using institutionalist and quantitative methods is employed to analytically and empirically identify the nature and scope of any economic dividend. By examining the relationships between spatial disparities, spatial economic policy and decentralization, the paper highlights the varied and uneven nature of these connections, which change direction during specific time periods, and the fundamental role of national economic growth in determining their outcome. While there is limited evidence of any economic benefit derived from decentralization, it is challenging to distinguish its effects from those of national economic growth, which decisively influences the pattern of spatial disparities and the scope and effectiveness of spatial economic policy and decentralization.",0
70,2,"The concept of an ""economic dividend"" has become increasingly popular in the UK and globally as a compelling argument for decentralisation in politics and economics. This trend reflects a broader shift from identity-based to economy-based rationales for contemporary forms of state modernisation and ""rescaling,"" as noted by Lobao et al. (2009) and RodriguezPose and Sandall (2008). The economic dividend is primarily about achieving allocative and productive efficiencies, as well as accountability and participation benefits, through decentralisation. This helps decision-making and co-ordination of collective action in support of economic development, according to Rodriguez-Pose and Gill (2005). In the late 1990s, proponents of Welsh devolution argued that it could improve economic performance and productivity, resulting in better jobs (Davies, 1999). Similarly, regional government in the English regions was seen as an ""economic imperative,"" addressing the ""economic deficit"" caused by persistent spatial disparities between London, the Greater South East region, and the rest of the UK (Murphy and Caborn, 1996).",0
70,3,"It is important to evaluate and reflect on the long-term impact of devolution on the UK economy, more than a decade after its implementation. However, little research has been conducted on this topic due to its complexity and difficulty in measurement. Challenges include finding suitable proxies, gathering sufficient and comparable data, untangling the effects of decentralization, and determining the causation between decentralization and broader economic and institutional changes. (Ashcroft et al. 2005; McGregor and Swales 2005).",0
70,4,"This study takes a multi-disciplinary approach, combining Economic and Political Geography, Local and Regional Development, and Political Science. It uses a geographically political economy to analyze institutional, political, and economic changes in the UK state over time and space. The study argues that examining the current ""economic dividend"" of devolution must account for past spatial disparities, spatial economic policies, and decentralization within the political-economies of specific nation-states. The historical dimension of analysis is crucial for understanding how past decisions and struggles shape current institutional structures and policy approaches.",0
70,5,"The paper aims to evaluate the evidence and lasting impact of the 'economic dividend' of devolution in the UK, a decade after the introduction of institutional and political reforms in 1997. Firstly, the paper addresses conceptual issues in relation to spatial disparities, spatial economic policy and decentralisation. Secondly, it outlines the historical context and development of the UK experience. Thirdly, the analysis concentrates on the period between 1984-2007 due to the scarcity of comparable data to examine the existence, extent and nature of any ‘economic dividend’ resulting from the interplay of spatial disparities, spatial economic policy and devolution in the UK. Finally, some conclusions and reflections are provided.",0
70,6,"The complexity and ambiguity of the inter-relationships surrounding the economic benefits of devolution suggests that situating the consideration of any 'economic dividend' within an understanding of the changing relations between spatial disparities, spatial economic policy and decentralisation is necessary. This includes identifying the different types of approaches to the issue – redistributive, free-market and growth-oriented – and their respective characteristics. It is important to distinguish between spatial economic policy and other forms of economic policy that have spatial implications. The recent debates concerning 'spatially neutral' or 'blind' policy that focuses on 'people' rather than 'place' are also recognized.",0
70,7,"Spatial economic policy and its organization are influenced by economic theories and their explanations for spatial disparities and adjustment processes. These theories are then translated into policy decisions within the institutional structures of each country's political economy. These transitions are often complicated and inconsistent, and are shaped differently in each national context. The purpose of this paper is to examine how these transitions have occurred in the UK. (Adapted from Pike and Tomaney, 2009)",0
70,8,"The interpretation of spatial disparities and formulation of spatial economic policy are closely linked to changes in geographical scale and institutional organization, shifting from centralization to various forms of decentralization. Devolution is just one aspect of decentralization, with different types varying in degrees of autonomy, reserved powers, and accountability. These differences are influenced by state and non-state actors, with varying motivations and expectations. Table 2 outlines the main types of decentralization. As decentralization is influenced by national political economies and historical contexts, it will differ between nation states.",0
70,9,"Conceptualizing the potential economic benefits and costs of decentralization is crucial in evaluating whether and what any ""economic dividend"" might entail. Table 3 presents some of the main issues, informed by the key arguments from fiscal federalism that focus on the vertical structure of the public sector, and allocative and productivity efficiencies, accountability, and participation. However, it is important to keep in mind that decentralization is a process and not a one-time event. Therefore, it is crucial to understand the timing and lag of any benefits and costs linked to decentralization. It is essential to differentiate between expected or likely benefits and costs and the unlikely, unexpected, or additional bonus or windfall economic impacts indicated by the term ""dividend."" Given that much of the discussion of the ""economic dividend"" focuses mainly on potential efficiency benefits, attention is required to address equity concerns regarding the distributional impacts of decentralization in economic, social, and geographical terms.",0
70,10,"It is difficult to produce a comprehensive assessment of the economic impact of devolution because spatial disparities, spatial economic policy, and decentralization are all interconnected. Therefore, analysts should take a more detailed and nuanced approach in order to account for the varied and differentiated nature of the economic dividend over time and space. The extent and nature of positive and negative impacts can vary greatly, and these impacts can spill over into political, social, and cultural domains. This is true for different nation states and territories.",0
70,11,"Spatial disparities have persisted in the UK since the 1930s, influencing the political economy, politics and policy. Initially, spatial disparities were attributed to economic inefficiency due to geographic overconcentration and centralisation of economic activities in the Greater South East, hindering national economic growth. However, more recent economic theory emphasizes the benefits of agglomeration in this region. Therefore, spatial policy should correct market failures to support this geographic concentration instead of redistributing its effects, to ensure overall growth and welfare are not harmed.",0
70,12,"The UK has a history of persistent spatial disparities, which has led to a long-standing focus on regional-scale economic policy. Various policies have been implemented over time, with intermittent periods of implementation and reflection on their effectiveness. Efforts to address high unemployment and poverty in specific areas were made during the inter-war period, while post-World War II policies were driven by efficiency and equity goals, as well as a desire to avoid territorial politics. The concept of 'Spatial Keynesianism' involved state intervention focusing on investment, stimulation of cumulative causation, and management of employment and demand in the regions. Relevant literature includes works by Barlow (1940), Harrison and Hart (1993), House of Commons (1995, 2003), Martin (1988), Gordon (1990), and Martin and Sunley (1997).",0
70,13,"Despite achieving some success in creating jobs and diversifying the economy in the Assisted Areas, the political and economic climate shifted away from Keynesianism and its spatial redistributive policies toward neo-liberalism in the wake of the stagflation crisis, industrial unrest and fiscal imbalances of the 1970s. This UK variant, championed by Margaret Thatcher's Conservative government from 1979, emphasized personal responsibility, free markets, and enterprise, thus providing a basis for dismantling regional policy during the 1980s. The turn toward neo-classical economics and the free market saw Keynesianism and regional policy as distortions and obstacles to rational and efficient decision-making among economic actors. Subsidies, which were viewed as economically inefficient and wasteful, were ineffective at addressing structural issues such as the lack of enterprise and innovation.",0
70,14,"Neo-classical theory focused on supply-side intervention to improve factors markets like labour skills, while Keynesian theory emphasized demand. In the UK, service sector growth in the Greater South East worsened spatial disparities, leading to reduced spending and a smaller map of eligible Assisted Areas under Thatcherism. This policy shift encouraged small enterprise and the attraction of international investment flows. Privatisation of formerly state-owned industries like coal, steel, and shipbuilding, however, had negative local impacts. Spatial policy was redirected to urban areas and resources were redirected to new bodies like Urban Development Corporations and Local Enterprise Agencies, away from local government (Martin and Tyler 1992).",0
70,15,"The institutional structures for formulating and delivering spatial economic policy in the UK have been shaped by its history as a highly centralised, multi-national 'union state'. Although there has been some administrative decentralisation to institutions such as the Scottish Office, Wales Office, Stormont Parliament, and Government Offices in the English regions, centralisation has remained the dominant approach to addressing spatial disparities through spatial economic policy. There have been some attempts at regional planning councils in England and the establishment of the Welsh Development Agency and Scottish Development Agency, but overall, the UK's institutional forms, powers, and resources remain variegated.",0
70,16,"The UK's devolution and constitutional changes from 1997 led to further democratic decentralization, resulting in a multi-level, polycentric state with a more distributed landscape of political power. However, compared to other European states, the UK is still highly centralized. The new arrangements established an asymmetrical hierarchy of powers and resources, ranging from Scotland's Parliament to the Mayoralty and Assembly in London. Territorial politics have since become more evident in varying degrees of differentiation in spatial economic policy and institutional organization, divergent from national, largely English-regions focused frameworks.",0
70,17,"The size and economic weight of England within the UK makes it challenging for the state to govern uneven development and growth, as it constitutes the majority of population and GDP. The political and institutional settlement for English regions has been left unresolved. The creation of RDAs in 1998 aimed to coordinate regional economic development and improve competitiveness. They were accountable both nationally and regionally through their sponsoring department and indirectly elected regional chambers.",0
70,18,"The expected boost in economic performance, known as the ""economic dividend,"" was highly anticipated with the increased autonomy, resources and political voice at the national centre. It was seen as a potential way to close the prosperity gap with London and the Greater South East sustainably. To achieve this, there would be a focus on distributive and productive efficiencies through designing and implementing policies suited to regional and local needs and creating a responsive, effective, and accountable governance system that could mobilise and shape collective action for developmental purposes. Despite the lack of faith in national government and politicians, the proposed ERAs for North East England were still rejected 3:1 in the sole referendum in late 2004 due to their weak powers and uneven enthusiasm and hostility in central government.",0
70,19,"The way devolution worked in Northern Ireland, Scotland and Wales influenced how regional projects unfolded. However, there was a lack of progress in regionalism and regionalisation in the English regions, so new spatial ideas emerged to tackle uneven development, such as resurgent cities and city-regions, localism led by local authorities, and pan-regionalism looking at issues across regions. After the coalition government was elected in 2010, there was a debate about the shift towards decentralization and localism, and the dismantling of the regional tier by closing Regional Development Agencies (RDAs) and abolishing Government Offices for the Regions.",0
70,20,"'Davies (1999) coined the phrase ""devolution is a process not an event"" which characterizes the gradual nature of decentralization. This article examines the presence of an ""economic dividend"" in the UK following institutional changes from 1997. It argues that an understanding of the relationship between spatial disparities, spatial economic policy, and decentralization is necessary and that these changes are influenced by previous eras of change. To do so, we analyze the historical paths of spatial disparities, spatial economic policy, and decentralization from 1984 to 2007 using local revenue data from 1987. We build upon Rodríguez-Pose and Gill's (2004) work and introduce quantitative measures of spatial economic policy.'",0
70,21,"The article aims to assess the 'economic dividend' of devolution in the UK, more than a decade after the constitutional reforms of the late 1990s. The study adopts a multi-disciplinary and geographical political economy approach to explore the complex relationship between decentralisation, spatial economic policy and changes in spatial disparities. The highly centralised system of the UK limits the emergence of any 'economic dividend', in contrast to other countries with more extensive fiscal decentralisation that have experienced positive effects under certain conditions. The study contextualises this relationship within the unfolding histories of institutional, political and economic change over time and space within the particularity of the UK state.",0
70,22,"The examination has disclosed various intriguing points. Firstly, the correlation between spatial incongruities, spatial economic policy, and fiscal decentralization is varied and uneven in the UK, with significant alterations in course during the period analyzed. Secondly, the importance of national economic growth is critical in explaining the progression of spatial disparities and the type and range of their relationship with specific forms of spatial economic policy and decentralization adopted. The predominance of national economic growth within the present growth-oriented methods of spatial economic policy has reinforced its crucial explanatory function, emphasizing the emergent tradeoff between nationwide economic growth and politically acceptable levels of spatial disparities (Martin 2008).",0
70,23,"Our findings indicate that the 'economic dividend' of devolution is likely to differ in form and intensity, and its presence may be temporary or sporadic in duration, despite being detectable. It appears that this is significantly influenced by the specific course of state institutional changes across various scales, as well as by national fiscal autonomy, economic growth, and central governments’ capacity and willingness to distribute resources. Nonetheless, it's feasible that our evaluation time frame was inadequate, and it may take over a decade for any 'economic dividend' to be seen. Furthermore, there should be improvements in data availabilities, and methodological advances are needed for the development of more proxies and indicators.",0
70,24,"The debate over the benefits of devolution in the UK remains ongoing among the various regions and nations, and has been further fueled by the Conservative-Liberal Democrat coalition government's formation in 2010. The implementation of devolution has been uneven, resulting in political controversy. For instance, a review by the Scotland Office in 2010 revealed that public spending in Scotland exceeded revenue by ￡75.8bn, leading to claims that devolution has provided Scotland with a measurable advantage. However, this assertion was disputed by the Scottish National Party. The issue of the economic and other benefits of devolution should be studied further, particularly through international comparisons.",0
71,1,"In this paper, a new method is presented to measure regional economic resilience in Europe that differs from previous approaches in three important ways. Firstly, it identifies the timing of regional downturns rather than assuming that all regions are affected simultaneously. Secondly, it quantifies both the duration and magnitude of economic downturns and recoveries. Thirdly, it assesses the ability of regional economies to resist economic shocks. The paper applies this method to several European countries, analyzing the different regional responses to various economic shocks since the early 1990s. Lastly, the paper evaluates the utility of this method for cross-comparative studies on regional economic resilience.",0
71,2,"The economic crisis that followed 2008 has impacted Europe in different ways depending on the region's vulnerability to economic shocks and their ability to recover (Jones et al, 2010; Martin, 2011; Hadjimichalis and Hudson, 2014). Regional economic resilience has become an important concept for evolutionary economic geographers to understand a region's capacity to recover and adapt to economic shocks (Davies, 2011; Fingleton et al, 2012; Martin, 2012). However, there is still a debate among academics about the clarity and conceptual framing of regional economic resilience due to the diversity of its applications and newness of evolutionary theorizing in economic geography (Bristow and Healy, 2014; Martin and Sunley, 2014; Boschma, 2014).",0
71,3,"Economic resilience is becoming more widely recognized as a crucial concept for policy-makers and practitioners who want to learn about why some places are better equipped than others to endure, and swiftly recover from, economic shocks. Dawley et al (2010) and CLES (2010) are a few sources that underscore this point. The significance of ""building a resilient economy"" is increasingly emphasized in policy debates concerned with economic development at local, regional, and national levels (Osborne, 2014).",0
71,4,"In recent years, there has been a significant increase in the number and range of resilience indicators and toolkits used by practitioners and policy-makers to assess the resilience of local and regional economies. However, the practical development of these measures has surpassed the conceptual understanding of resilience, leading to a lack of consensus on measuring resilience and a potential dilution of its usefulness. Outdated indices have been inaccurate in predicting economic resilience, making it essential to establish reliable measures to determine actionable variables and identify resilient economies. Rushing to identify the characteristics of resilient economies may result in losing an objective means to differentiate between economies that can withstand economic shocks and those that cannot.",0
71,5,"The aim of this paper is to contribute to the discussion on regional economic resilience by creating an approach to measure it through a comparison of the post-2008 global financial crisis effects on European regions. The approach focuses on post-shock outcomes, using available methods to date regional business cycles and capture regional differences in timing, amplitude, and duration of downturns and recoveries. The sections of the paper include an examination of the challenges of operationalising resilience, a description of our approach's unique features, key results from applying this approach to the recent economic crisis in Europe, and reflections on the usefulness of the approach. The conclusion identifies how this approach can advance cross-comparative research on resilience.",0
71,6,"Resilience, according to evolutionary theorists, should be understood as a multi-dimensional and ongoing process. This involves not only understanding the shock and the region's vulnerability to it, but also its ability to resist the shock, the strength of its firms and institutions in responding to the shock, and the extent and nature of the economy's recovery. The complexity of resilience is a critical issue in measuring it, as there is a tendency to conflate resilience outcomes and resilience capacities. This has been a problem in much of the resilience literature (Bristow and Healy, 2014).",0
71,7,"It is crucial to differentiate between a region's revealed resilience and its resilience capacities when measuring resilience. Adaptive capacity indicators do not show resilience, but they reveal the adaptive mechanisms that allow a regional economy to be resilient. These mechanisms should be examined to understand the variability of resilience outcomes. Structural and behavioral factors and attributes influence these mechanisms, and their relative importance is subject to debate. Quantitative assessment and a more localized and qualitative ontology are likely necessary.",0
71,8,"The metric for identifying the reference state needs to be carefully selected. To measure resilience as performance or outcome, comparable economic indicators such as GDP and employment must be identified. It is important to understand how to deal with mixed results and trade-offs and to determine the perceived resistance or recovery state. As shocks may permanently change the composition of economies or the behavior of key actors within them, this can be challenging. Another challenge is determining whether resilience is being analyzed in absolute terms or relative terms. Resilient outcomes may be judged in comparative context or relative to a region's own reference state or norms. (Martin, 2012).",0
71,9,"Identifying when an economic shock occurs is a fundamental challenge for any study on economic resilience. The argument that economies are continually reacting to changing circumstances and never in a state of equilibrium implies that they consistently face minor and major shocks. Only when these shocks reach a certain magnitude or happen in a specific context do their effects become observable. Economic shocks are similar to physical earthquakes, with small regular tremors going unnoticed except by sensitive instruments, and more substantial earthquakes causing varying degrees of damage, depending on the preparedness of the impacted area.",0
71,10,"Identifying the onset of a system-wide shock in a complex setting is challenging. Looking for a single incident to date the shock can confuse cause and effect. For instance, the collapse of Lehmann Brothers is often cited as a key point in the recent financial crisis, but its roots lie in the sub-prime mortgage crisis following the collapse of the US housing bubble. The effects of the shock reverberated across the world through complex interactions and second and third order effects on financial markets, trade links, and behavioural changes. These changes were driven by citizens' modified consumption and savings behaviour based on their expectations of the future.",0
71,11,"There is a challenge when it comes to determining the time period required for resilience outcomes to become apparent. Previous research suggests that there is no clear-cut answer. The study of regional economic resilience in the US by Hill et al (2011) considers a region to be resilient if it returns to its previous growth trajectory within four years. There are concerns about the connection between short-term resilience to shocks and a region's long-term regional development pattern. Martin and Sunley (2014) argue that short-term measurements of resilience as a 'bounce-back' to a pre-shock state or norm are necessary for comprehending long-term regional growth paths and development trajectories. Therefore, understanding short-term resilience results could be critical to comprehending long-term regional convergence and divergence patterns.",0
71,12,"There is a challenge in terms of the scale of analysis, with a lot of analysis being done at the national level. However, there is now more interest in the economic resilience of smaller sub-national units, like regions, cities, or other administrative units. This is because resilience to economic shocks may differ depending on the scale of analysis. Just because the national economy is resilient, it doesn't mean that the smaller regions and localities will also exhibit the same level of resilience, especially considering their unique characteristics and development paths.",0
71,13,"The measurement of resilience has been a challenging issue for researchers. Some have tried to create a set of indicators but doing so may merge cause and effect. To avoid data correlation in the future, it is necessary to determine a method of measuring a region's resilience to economic shocks accurately.",0
71,14,"We opted to utilize two sets of data: the employment levels and the Gross Domestic Product (GDP). These data are consistently available and comparable across the EU. Employment is a more significant measure than GDP as it calculates the number of employed individuals and is less prone to alterations. Moreover, it provides a social value as it is considered a sign of the economy's wellbeing. Nevertheless, since GDP is a standard measure of economic prosperity and recession, we also consider it. Using real GDP data (in 2005 constant prices) offers a better view of income and output resilience over time.",0
71,15,"To determine the resilience of regions, we must first identify their business cycle turning points. Then, we can determine the amount of GDP or employment lost during the peak and trough of the cycle. Our method treats each region as a separate time series and dates each business cycle turning point, allowing us to measure resilience by calculating the amount of output or employment lost during downturns and the time it takes to recover. This approach expands upon Sensier and Artis (2014), which dates employment cycles of countries within the UK, and improves upon Martin's (2012) approach, which assumes that all regions have the same turning point dates as the national employment series.",0
71,16,"It has been decided to use the NUTS 2 regional classification of Eurostat to define regions for our work. Despite the fact that there may be some difficulties with this approach (McLeod, 2001), having access to uniform datasets at this level is considered more important. Additionally, for comparative reasons, data at the NUTS 3 territorial level is also used and referred to as 'local' to distinguish it from the regional scale.",0
72,1,Economic growth is a topic that has been extensively researched in specialized literature. It emphasizes the modifications that transpire in improving macroeconomic outcomes that are not expressed on their own but rather interconnected with factors that influence them. The objective of this article is to create a model of economic growth for the European Union members by considering factors such as the economic freedom index and the index of economic progress.,0
72,2,"We have identified two main objectives for our study, which are measuring the strength of the relationship between GDP per capita and the index of economic freedom, and comparing it to the social progress index. Additionally, we aim to model growth rates of these indices for the EU Member States Union. To explore these objectives, we have employed various research methods including descriptive analysis, analysis of variance, correlation analysis, and multiple regression analysis.",0
72,3,"We created a regression model that examines how the economic freedom and social progress index impact the growth of GDP per capita in EU member countries. By analyzing variances, we determined that there are significant variations in the average GDP per capita in European countries based on their EU membership status.",0
72,4,"'Since Adam Smith, economic freedom has been proven as the most effective way to achieve prosperity in the liberal tradition. Nonetheless, in order for it to be considered a progress, it must be connected with social and spiritual progress. In a given time and location, economic growth is reflected by the actual increase of economic indicators such as GDP.'",0
72,5,"Economic growth is characterized by changes occurring in the increase of macroeconomic results, which are closely connected with its determinants. Numerous theoretical and empirical studies suggest that various factors contribute to this process, including investment, physical and human capital, innovation, geography, political and legal institutions, and macroeconomic conditions. Scholars such as Bassanini and Scarpetta (2001), Petrakos and Arvanitidis (2008), and Moral-Benito (2009) have examined these determinants and their impact on enhancing economic performance.",0
72,6,"Various methods have been employed to measure economic freedom. Several indicators that attempt to measure economic freedom have been created, such as the Economic Freedom of the World by the Fraser Institute, the Index of Economic Freedom by the Heritage Foundation and the Wall Street Journal, and the World Survey of Economic Freedom by Freedom House. These indicators have a few similarities as they commonly use fundamental aspects of economic freedom in their criteria such as protected private property rights, adherence to the rule of law, the freedom to trade, and limited government involvement, as mentioned by Iacobu?? and Gagea in 2010.",0
72,7,"The Index of Economic Freedom from Heritage Foundation & Wall Street Journal is used to analyze economic freedom in this paper. The indicator is an aggregate of 10 components grouped into 4 categories: Rule of law, Limited government, Regulatory efficiency and Open markets. Each component is scored from 0 to 100, with a higher score indicating a higher degree of freedom. (Heritage Foundation, 2016).",0
72,8,"The dataset comprises of data on 40 European nations. We analyzed variables such as the European Region the country belongs to (""Eastern Europe"", ""Western Europe"", ""Northern Europe"", and ""South of Europe""), membership of the European Union (""Yes"", ""No""), GDP per capita (2014), GDP per capita (2013), Index of Economic Freedom, Index of Social Progress, and GDP per capita growth rate. The data was obtained from sources such as WorldBank, Heritage Organization, and Social Progress Imperative.",0
72,9,"Variance analysis is a statistical technique used to study the variability of a variable in relation to various factors affecting it. The ANOVA technique decomposes the total variance of a dataset into its source components (causes) and compares them to determine if these factors significantly influenced the variable. To analyze the effect of EU membership on the GDP per capita, we used ANOVA. We used correlation analysis to examine the relationship intensity between the economic freedom index and GDP per capita, and the economic freedom index and the index of social progress. A multiple regression model was then used to simulate economic growth for European countries based on the economic freedom index and the index of social progress. This model shows the strength of the relationship between the considered factors and their impact on the studied phenomenon. To validate the econometric model, we checked for the fulfillment of assumptions in both the deterministic and stochastic components. The stochastic component tests assumptions such as the null influence of the error term in the series, the homoscedasticity hypothesis, the independence hypothesis, and the normality hypothesis.",0
72,10,"Economic growth is a multifaceted procedure that aims at the entire economic system and its interactions. It emphasizes the transformations that arise from the enhancement of the macroeconomic outcomes, which are closely associated with their underlying factors.",0
72,11,"The objective of this research was to examine the correlation between economic growth and the index of economic freedom as well as social progress index. Upon conducting a preliminary assessment of the variables' distributions for 40 countries within the sample, we determined that nations in Southern and Eastern Europe recorded lower scores in comparison to other regions. Hence, a large proportion of these countries scored below 63 for the index of economic freedom and lower than 78 in the social progress index.",0
72,12,"We conducted a test to establish if the GDP growth rate is affected by the two indices assessed in our analysis. We utilized a logistic regression model to determine this relationship, but the variables integrated into the model were not statistically important. Consequently, we can conclude that the direction of the GDP per capita growth rate is not influenced by the two indices we analyzed. Other unaccounted-for factors can impact this growth rate.",0
73,1,"This paper introduces the concept of Economic Space, which expands the capacity of economic and financial modeling. Economic Space defines economic variables as functions of time and coordinates, enabling us to treat economic and financial relationships similarly to mathematical physics equations. It facilitates the study of economic models on discrete and continuous spaces with varying dimensions. The Economic Space dimension is determined by the number of risks measured simultaneously. The paper provides several examples of modeling with Economic Space, such as option pricing and the derivation of Black-Scholes-Merton equation on n-dimensional Economic Space, and Markov processes with the derivation of Fokker-Plank Equations. Usage of Economic Space allows approximations of economics and finance akin to physical kinetics and hydrodynamics, and derives Wave Equations for Economic and Financial variables.",0
73,2,"Economics and finance are highly complex systems, similar to theoretical physics. However, the phenomena studied within these fields are quite different. In recent years, the field of Econophysics has made significant contributions to better understanding and modeling economic and financial features using statistical physics methods (Mantegna & Stanley, 2000; Roehner, 2002; Stanley, 2003; McCauley, 2006; Special Issue, 2008; Schinckus, 2013). The methods and models developed in theoretical and statistical physics may also prove useful for accurately describing economic and financial processes. This represents an initial step towards modeling economic processes with similar accuracy and understanding to that of physical processes. We view Econophysics as a means of adapting existing methods and schemes from theoretical and statistical physics to economic and financial modeling while still adhering to relevant economic and financial laws. This approach may ultimately require a re-evaluation of the foundational basis of most existing models developed within theoretical and statistical physics.",0
73,3,"We examine the economic comparisons to physical concepts that could prove beneficial for financial and economic modeling. Among these physical concepts, the space-time issue is a widely-used and general one. We pose a straightforward query - can we establish a correlation between the concept of space-time in physics and economics to reap its potential benefits? In response, we suggest introducing the concept of Economic Space as a parallel analogy.",0
73,4,"We propose the concept of Economic Space as an expansion of the existing notion of matter in economics, with the assumption that it adheres to economic phenomenology. This concept allows economic variables to be seen as functions of time and Economic Space coordinates, making it possible to apply mathematical physics equations and methods of theoretical physics to economic modeling. However, it is important to note that the lack of conservation laws and symmetries in economics and finance make such differences between economic and physical systems crucial, requiring that theoretical physics methods be modified to suit economic and financial phenomenology. We believe that introducing Economic Space could revolutionize economics and establish a strong foundation for using theoretical physics in economic and financial modeling. This study is motivated by the idea that space-time constitutes the basis for theoretical physics and the description of nature.",0
73,5,"We present the concept of Economic Space, which aims to examine economic variables of both economic agents and macroeconomics as functions of time and coordinates in Economic Space. These variables include Supply and Demand, Loans and Debts, Labor and Taxes, Production Function, Capital, and more. It's important to note that our definition of Economic Space is not related to spatial econometrics (Anselin, 2009), and our approach diverges from agent-based economics (Judd & Tesfatsion, 2005). We propose that various economic states can determine distinct ""intrinsic"" Economic Spaces, which offer alternative approximations of economic and financial processes. The existence of such spaces, and the ability to describe economic variables as functions of time and Economic Space coordinates, allows for the use of functional analysis, stochastic functions, and modern methods of mathematical physics for economic and financial modeling.",0
73,6,"The introduction of Economic Space into economic modeling presents two challenges: firstly, how to define Economic Space, and secondly, how to measure the coordinates of economic variables and partition them over a particular Economic Space. To address these issues, we propose utilizing commonly used methods within economics. Specifically, we reference Lee (1999), as well as studies on economic and financial risk management conducted by BIS (2011) and BIS (2013). Such risk management studies principally focus on risk ratings for banks and corporations, which are offered by various international rating agencies (e.g. Fitch (2006), Moody's (2007), S&P (2012), and DBRS (2013)). Thus, we suggest that Economic Space and modeling within it can be developed using practices typically associated with risk rating.",0
73,7,"Risk ratings procedures distribute economic agents, such as companies, corporations, and banks, over a finite number of risk grades. These grades act as points in a discreet space, with each economic agent being assigned coordinates based on their risk rating. The process of risk ratings estimation is akin to measuring the coordinates of economic agents on discreet space. One-dimensional discreet space can be used to represent a single risk rating, while n-dimensional discreet space can be used to represent the simultaneous estimations of ratings of n risks of economic agents. This allows for existing risk ratings practices to be viewed as a method of distributing economic agents on discreet space, referred to as Economic Space. Economic variables can then be defined as functions on Economic Space based on the coordinates of the associated risk ratings or economic agents.",0
73,8,"In order to extend the BSM equation and improve option pricing modeling, we examine option pricing theory on n-dimensional e-space Rn. This reveals the need to consider changes in the main risks that determine the current e-space representation. The random dynamics of these risks during the time to expiration requires transformation of the pricing equation into one with different axis on e-space. This effect may explain discrepancies between predicted and observed option prices, and accurately describing these effects can improve pricing accuracy.",0
73,9,"The concept of e-space reveals the intricate internal workings of financial and economic systems, with challenges arising from econometric issues, risk selection, and measurement of e-particle distributions. Currently, there is a lack of risk rating methodologies capable of distributing economic agents on Rn. Therefore, cooperation among various industry players is vital to establish economic agent distributions on e-space with one or two dimensions. It is crucial to unify risk rating methodologies, develop methods to measure and compare different risks, and choose the most valuable risks to address this issue. Our proposal is that even a basic e-space macroeconomic model can enable observations of economic variables, visualize macroeconomic distributions, and describe e-space states such as Inflation, Financial Markets, Derivatives Markets, among others.",0
12,8,"The absolute scores for the consumption of water and the almond solution, on which the ratios were based, are presented in the top panel of Fig. 2. The pattern is the same for both reinforcers, with both NPre groups drinking less water than almond, but both Pre groups drinking less almond than water. There was substantial within-group variability in the absolute consumption scores, and an ANOVA on those for the consumption of almond, with preexposure condition and reinforcer as the variables, showed only a marginally significant main effect of preexposure, F(1, 28) = 3.03, p = .09, ηp 2 = .10; other Fs < 2. The water scores showed no significant differences (all Fs < 1).",1
12,9,"The subjects were 32 male na?ve Wistar rats (Janvier, France) with a mean body weight of 355 g at the start of the experiment. The housing, general maintenance, solutions, and apparatus were the same as we described for Experiment 1, with the exceptions mentioned below.",1
12,10,"As in Experiment 1, the subjects in the Pre groups received eight sessions of preexposure to almond, and the NPre subjects received unflavored water, prior to conditioning with either the almond + fructose compound or the almond + maltodextrin compound, and then the choice test. Food was available throughout, except for the 30-min duration of each conditioning and test session. In our previous experiments investigating preferences in nonhungry rats (Garcia-Burgos et al., 2013), we gave two conditioning–test cycles (i.e., after the first test, the rats received two further sessions of conditioning, followed by another test), and this procedure was followed here.",1
12,11,"As in Experiment 1, the rats drank readily during preexposure, and we found no differences among the groups. The mean consumptions (in grams) of almond over the course of this phase were 8.2 for group Pre/M and 8.3 for group Pre/F. The mean consumptions of water were 8.4 for group NPre/M and 8.2 for group NPre/F. As we assessed by an ANOVA with preexposure and reinforcer as the variables, no significant effects were present, largest F(1, 28) = 1.54, p = .225, ηp 2 = .05.",1
12,12,"Mean preference ratios pooled over the two tests for each group appear in Fig. 1 (right; the groups are now labeled TTT, for being only thirsty in all stages of the study). Scores were somewhat higher for maltodextrin than for fructose, and, in striking contrast to the results of Experiment 1, the preexposed groups showed a higher, rather than a lower, preference for almond, irrespective of the reinforcer. This reversal of the latent inhibition effect was quite unexpected. An ANOVA conducted on the average preference ratios, with preexposure and reinforcer as the variables, yielded significant main effects of preexposure, F(1, 28) = 5.17, p = .031, ηp 2 = .16, and of reinforcer, F(1, 28) = 5.48, p = .027, ηp 2 = .16; the interaction was not significant (F < 1).",1
12,13,"Although the patterns of preference scores were the same for fructose and maltodextrin, inspection of the absolute consumption scores shows marked differences. The lower panel of Fig. 2 shows the group means for consumption of almond and water, pooled over both test trials. For maltodextrin, consumption levels of water and almond were approximately the same in subjects given no preexposure, but more almond than water was consumed by the preexposed animals. For fructose, on the other hand, the preexposed subjects were the ones to drink equal amounts, whereas the nonpreexposed subjects drank more water than almond. Rats conditioned with maltodextrin drank more almond than did those conditioned with fructose. An ANOVA on the scores for consumption of almond, with preexposure condition and reinforcer as the variables, showed a significant effect of the nature of the reinforcer, F(1, 28) = 6.48, p = .017, ηp 2 = .08; the main effect of preexposure, F(1, 28) = 2.57, p = .120, and the interaction (F < 1) were not significant. Water consumption, by contrast, was sensitive to the effects of preexposure, with preexposed subjects drinking less than nonpreexposed subjects. An ANOVA showed a significant effect of preexposure, F(1, 28) = 6.03, p = .021, ηp 2 = .18; the main effect of reinforcer, F(1, 28) = 1.54, p = .225, and the interaction (F < 1) were not significant.",1
12,14,"Latent inhibition is a powerful and well-documented effect, and any failure to observe retarded conditioning after extensive preexposure to the CS requires attention. The absence of latent inhibition in rats conditioned with sucrose as the US and tested when not hungry (Garcia-Burgos et al., 2013) prompted the hypothesis that a preference supported by flavor–taste learning is not susceptible to latent inhibition. In the present experiments, we attempted to test this hypothesis by making use of maltodextrin and fructose as the reinforcers and by testing preexposed and nonpreexposed subjects either in a state of hunger (Exp. 1) or when sated (Exp. 2). On the basis of the evidence presented in the Introduction, it was supposed that maltodextrin would engender flavor–nutrient learning, the effect of which would be evident in the preference shown by hungry animals, and that fructose would engender flavor– taste learning, capable of producing a preference even in nonhungry animals.",1
12,15,"The results of the experiments did not support these suppositions. We found, for subjects not given preexposure to the CS, that both USs were capable of generating a preference when the rats were hungry during the test, and when the rats were not hungry, neither produced a clear preference. If the proposed distinction between flavor–taste and flavor–nutrient learning is to be maintained, we must suppose that, with the stimuli and procedures used in our experiments, both USs are capable of producing flavor–nutrient learning, and that the effect of flavor–taste learning is weak for both reinforcers. This would impose limits on the strategy of using these two reinforcers to distinguish between two kinds of flavorpreference learning, suggesting that, at least under the conditions used in the present experiments, they may be functionally equivalent (for related results in the case of blocking, see González, Garcia-Burgos, & Hall, 2014).",1
12,16,"To an extent, therefore, these findings confirm those of Garcia-Burgos et al. (2013), from their experiments using sucrose as the US, in that latent inhibition was evident in flavorpreference conditioning when the subjects were hungry during the test, but not when they were sated. An obvious interpretation of this pattern of results, at least at first sight, is that hunger promotes the expression of a preference (whatever the US), allowing conditioning to be seen clearly, and thus, the effect of preexposure to be observed. It might be argued that the absence of latent inhibition in the study by Garcia-Burgos et al. was merely a consequence of a low sensitivity to appetitive conditioning in nonhungry animals. But this cannot explain the results of the present Experiment 2, in which nonhungry rats showed a significant reversal of the latent inhibition effect. Other factors must be at work.",1
12,17,"In summary, the use of maltodextrin and fructose as USs does not allow the clean separation of flavor–taste and flavor– nutrient learning that we had hoped for, but the manipulation of motivational state has a marked effect on latent inhibition. With both USs, preexposure retards the development of a conditioned preference for animals tested in a state of hunger, consistent with the conclusion that both generate flavor–nutrient learning that is susceptible to latent inhibition. When the subjects are not hungry during the test, there is an apparent reversal of latent inhibition. The source of this effect is complex and debatable, but we have offered a possible explanation that is consistent with the original proposal that flavor–taste learning is not susceptible to latent inhibition.",1
12,18,"In 1998, Claire Schmais delivered the annual Marian Chace Foundation lecture, and it was my distinguished honor as her colleague and friend to introduce her. ‘‘Try to imagine the world of dance therapy without Claire Schmais,’’ I said then: ‘‘It is almost incomprehensible’’ (White as quoted in Schmais, 1999, p. 5). Those words resonate in a new way today. Sadly, the time has come for us to advance our field without the direct benefit of Claire’s foresight, brilliance, and determination. Without her guiding presence, we rely from now on, instead, on the written word—her groundbreaking scholarship and other writings—along with the living testimonies of her peers and the many students she trained with such insight and devotion.",1
12,19,"At the 22nd annual conference of the American Dance Therapy Association (ADTA), Sharon Chaiklin, Beth Kalish, Claire Schmais, and I, presented papers addressing how our personal lives shaped our work as dance therapists (ADTA Monograph, 1987). Having had but a single conference call to set up a format, we had not disclosed to one another what we were going to say or do. As a result, we panelists listened to each other with particularly rapt attention. We had already been working together for 23 years while establishing the ADTA and we certainly knew something of each other’s personal backgrounds. It was, however, quite unusual for us to hear such candid personal reflections on our experiences and feelings.",1
13,1,"Claire’s own words from that panel presentation painted such a vivid picture of her and the factors that motivated her actions that I choose to quote them extensively here. In a partnership with her that lasted more than half a century, I always trusted, respected, and looked to Claire for intellectual acumen and her extraordinary ability with words. Sharing her self-portrait seems most appropriate now. Indeed, revisiting Claire’s talk has been enlivening for me, as it has enabled me to hear Claire’s voice again, as she described herself so poignantly.",1
13,2,"Claire’s talk illuminated her strength of character, determination, social commitment, vision, and force of personality. I have identified a context for what Claire said of herself in each of the selected excerpts below.",1
13,3,"For many, many years Claire belonged to a dream interpretation group, which provided her important insights during difficult times and sustained her meaningfully after she retired from teaching. She cited one of her dreams as the ‘‘inspiration’’ behind her talk:",1
13,4,"I dreamt that a salesclerk at Macy’s urged me to buy a gray flannel suit, but I insisted on an Irish tweed—and that is how I see myself—sturdy, rebellious, warm on the inside, and a bit rough on the outside. As an outgrowth of this dream, I decided to share with you some very personal stuff about how two strands of my past have intertwined. My active temperament and my activist role. (ADTA, 1987, p. 5)",1
13,5,Claire was forward thinking in seeking out different modalities for self-reflection. She applied a discovery made through hypnosis toward the examination of her history in dance,1
13,6,"My first memory … is wriggling free from my mother’s arms. It is no accident that struggle, movement, and freedom have been salient metaphors in my life. When I was eight years old I went to an afterschool dance class. There I discovered a place that was congruent with my nature. Instead of being admonished to sit still, I was encouraged to move, to dance out my fanciful dreams and my darkest fears. The teacher, Edith Siegel, was my first model of a free spirited woman and a political activist. We danced war, poverty, peace, and freedom … When those dance classes were over, I asked my parents to enroll me in a dancing school. They said, ‘No.’ I remember crying all day and into the night. They still said, ‘No.’ Not because they were mean spirited, but because they were poor. They wanted to see me get a job with security, minimally a pension. That day was a turning point when I realized that my parents could not meet my needs. In order to be myself I would have to forego parental approval. (ADTA, 1987, p. 5)",1
13,7,"An accomplished athlete, Claire had met her husband on a handball court. She openly disclosed aspects of her temperament through what she had to say about how her athleticism had enabled her to learn to compete and advance her ideas:",1
13,8,"During the establishment of a DMT association and even after its formation, many battles ensued with Claire’s full force behind them. Plans had been made to hold the ADTA’s third annual conference, in 1968, in Chicago. Given the violent crackdown on protests at the Democratic presidential convention in Chicago that summer, Claire implored the ADTA board to relocate the conference in solidarity with the protestors. And so it was: The annual conference was held that year in Madison, Wisconsin, instead. The move came, in effect, as a testament",1
13,9,"I used my energy and I dispelled my anger on the playing fields of Crotona Park in the South Bronx. In the process of becoming Girls Bronx Handball Champ, I learned to compete, to hit out directly and to put my weight behind my thrust. In later years I never felt fulfilled playing tennis, where form predominates. I like battling directly with my hand. (ADTA, 1987, p. 6)",1
13,10,"Claire both interned with Marian Chace at St. Elizabeth’s Hospital in Washington, and joined in Chace’s 3-week workshop at Turtle Bay Music School in New York City, in l961. In that course with her were Beth Kalish, Deborah Thomas, and Arlynne Stark. During the workshop, Chace described two types of patients—manic and depressed—and asked that each participant pick a type and move as she thought that person would move. Claire chose to evoke mania. She knew herself well, and this choice was indicative of her intense energy level:",1
13,11,"As a young dance therapist it is no accident that I was happiest working with young children, acting-out adolescents, bizarre schizophrenics, and rambunctious elderly. Depressed patients gave me a backache. (ADTA, 1987, p. 6)",1
13,12,"‘Introduction to Dance Therapy Using Effort-Shape Movement Analysis’’ workshops.], Elissa and I, along with Martha Davis, pioneered the first graduate degree program in dance therapy [as referenced above]. The central focus of my teaching was, and is, that to use yourself as a therapist you must be fully known to yourself [See Schmais & White, 1969]. Fortunately, and sometimes painfully, I realized that to teach that concept I had to model it just as I am trying to do today. (ADTA, 1987, p. 6)",1
13,13,"Claire explained in her talk that her ‘‘personal style and professional life intersected’’ (ADTA, 1987, p. 6) in her leadership role in the ADTA. In 1965, she chaired and organized the first annual meeting and conference of ADTA and did so again in l970. It is clear that she was a catalyst, and her list of ‘‘firsts’’ is quite long. She shared background on the ADTA’s origins:",1
13,14,"Back in the sixties, I pushed for an association of dance therapists. Given my political experience, I knew the power of an organized group. When I first raised the issue, Marian Chace would not hear of it and I was summarily dismissed. I continued to prod [See Schmais & White, 1996]. In l965, when the time was ripe the idea took hold. (ADTA, 1987, p. 7)",1
13,15,In that year Claire was chosen to be the chair of a steering committee to form a dance therapy association—an idea that had been discussed during the previous year. Her foresight and perseverance helped determine a course of action.,1
13,16,"Once the ADTA was incorporated, Claire was elected the first Chair of the Education Committee, a role she filled with true prescience. In 1968, she organized a 1-day conference on dance therapy’s research potential with Bonnie Bird, a Past Chairman of CORD, the Committee on Research in Dance (Bird, 1970). Held on November 10, at New York City’s Postgraduate Center for Mental Health under the auspices of its Director and Associate Director of Research, this was the first event ever organized around research on dance therapy. Among the speakers and presenters representing DMT were Sharon Chaiklin and Mary Whitehouse who presented their work on film. Among other luminaries present from various disciplines were: Valerie Bettis and Rod Rodgers, as well as Irmgard Bartenieff, Martha Davis, Judith Lynne Hanna, Judith Kestenberg, and Alan Lomax.",1
13,17,"In the early 1970s Claire was a member of the first committee to formulate a DMT Code of Ethics. She also became the first chair of the Approval Committee whose initial task was to write the guidelines for approving graduate programs. Once Hunter College’s DMT program came into existence, a number of other graduate programs followed. Claire noted:",1
13,18,"In the early seventies I raised the unpopular notion of approval for graduate programs. This was met with outrage and resistance. It took about five years to become accepted. In my next foray I urged that we adopt a single title for the graduate degree. I thought it should be an MS in Dance Therapy; Beth [Kalish] thought it should be in Movement Therapy. This time I lost the battle but won the war. The directors of graduate programs, after such sturm und drang, agreed to one title, ‘Dance/Movement Therapy.’ And thus we have a professional identity in academe. (ADTA, 1987, p. 7)",1
13,19,"Claire’s commitment to education and to students of DMT resulted in so many consequential endeavors. Her publications on group process and development (Schmais 1981, 1998) and healing processes in dance therapy (Schmais, 1985; Schmais & Diaz-Salazar, 1998; Schmais & Felber, 1977) are seminal examples of theoretical considerations. Her writing talents also equipped her well to serve twice as co-editor of this journal, which she further served as a member of its editorial board for many years.",1
13,20,"It may be a testament to Claire’s remarkably full life and countless achievements that I have written this much without yet mentioning her extraordinary devotion to her many students. Claire’s teaching was an art which was constantly and creatively changing depending on who was in her class (Schmais, 2004). Her concern, caring, and attention to each and every one of her students was a lesson witnessed by all of us who had the opportunity to work alongside her. What she hoped to instill in each of us was to be true to ourselves. Claire’s example was such a strong guide that it could not be more apt that to draw on her own words, in closing:",1
13,21,"The need to better understand, represent, advocate and provide quality healthcare in the field of sexuality and disability continues as an international challenge. This global endeavor brings the world a little closer to the issues and the ability to share the work and ideas on this journey. Most important with any challenge is the fact that our effort highlights that there are opportunities that surround us. Opportunities that can help others. Opportunities that can be undertaken. Opportunities to perhaps make the world a little better. And so we continue…",1
13,22,"The journal of Sexuality and Disability remains a professional home for many, and a place of professional rejuvenation and exploration for still more. If you are new to the journal, we give to you a ‘‘BIG WELCOME’’ to participate as a reader, author, academician, clinician, educator, service provider, researcher, advocate, and individual or individuals seeking information. Over decades of contribution to the literature on sexuality and disability, we have been a part of the growth and understanding. Now most importantly, we continue to be a part of change in how we look and examine the topic, the need, and the response in terms of best practice-evidence based approaches. With the intelligence, experience, motivation and commitment from our authors, readership, peer reviewers and editorial board, and resources with guidance from the Springer Staff, our journal’s mission continues to be strong and meaningful.",1
17,46,"According to language control models, several of the described processing stages could be a locus of language control. Yet, there seems to be a divide among these models with respect to the functional locus or loci that they propose. Some models assume that the functional locus of language control can be found at the concept level (La Heij, 2005; Poulisse & Bongaerts, 1994; see also Schwieter & Sunderman (2008) with highly proficient bilinguals). La Heij (2005), for example, assumed that language cues at the concept stage allow for additional activation of the target representation in the correct language. Consequently, target representations in the corresponding language should be selected.",1
17,47,"Other models assume that the functional locus of language control can be found at the lemma level (Declerck et al., 2015a; Grainger et al., 2010; Green, 1998; see also Schwieter & Sunderman (2008) with second language learners). In the modified ICM (Declerck et al., 2015a), bilingual language processing starts with the activation of language schemas, which are mental devices that are implemented to achieve task-specific goals, like speaking in a certain language. Once the lemmas have been activated by their respective concepts, each language schema activates their corresponding language node and an inhibitory competition process occurs between these language node(s). In turn, the language nodes inhibit lemmas of the other language. Finally, competition between the target lemma and the translationequivalent lemma results in selection. So, language control occurs between language nodes and between lemmas (both located at the lemma level) according to this model. Furthermore, this model also includes stronger connections from concepts to the respective L1 lemmas than from concepts to their respective L2 lemmas for second language learners (cf. Kroll & Stewart, 1994; see also Schwieter & Sunderman, 2008).",1
17,48,"Since the modified ICM (Declerck et al., 2015a) is based on the original ICM (Green, 1998), a similar progression is assumed in the latter. The main difference is that language control is presumed to occur between language schemas and between lemmas, according to this model, instead of between language nodes and lemmas in the modified ICM. Interestingly, the schemas proposed to control for language interference are similar to those used for more general cognitive control according to the ICM. Hence, the ICM assumes that language control occurs at the lemma level and outside of language processing",1
17,49,"In the BIA + (Dijkstra & van Heuven, 2002), a similar process is proposed to occur during comprehension-based language control as in the BIA, with the most important distinction being the differentiation between the word identification system, which entails linguistic influences, and a task/decision system, which entails nonlinguistic task schema influences. The latter being a processing stage outside of language processing.",1
17,50,"Thus, several loci of language control have been proposed, ranging from the concept level, lemma level, and a processing stage outside of language processing (i.e., cognitive control processes). Interestingly, several models have even proposed more than one locus of language control (Declerck et al., 2015a; Dijkstra & van Heuven, 2002; Green, 1998), both within and outside of language processing. In the next sections we will discuss whether there is empirical evidence for any of these potential control loci, and some that have not yet been considered (i.e., phonology and orthography).",1
17,51,"Since several models of language control proposed an important role for the concept level (Declerck et al., 2015a; La Heij, 2005; Poulisse & Bongaerts, 1994; Schwieter & Sunderman, 2008), either by differently weighed connections to the corresponding lemmas or as a functional locus of language control, one would assume that manipulations on this level would affect language switching. Declerck et al. (2015a) found that concepts and concept activation can have an impact on switch costs. In this study, German–English bilinguals could prepare through predictable information for several aspects of language switching (i.e., language, concept, or both). When concepts could be prepared, smaller L1 switch costs were observed than when concepts could not be prepared, whereas the effect of concept preparation on L2 switch costs was much smaller. This finding was explained by a stronger connection between concepts and L1 lemmas than between concepts and L2 lemmas (Kroll & Stewart, 1994). Hence, when a concept could be prepared, more activation was sent to the respective L1 lemma than to the L2 lemma, making it easier to select the former in a switch condition. These differently weighed connections have been implemented in several language control models (Declerck et al., 2015a; Grainger et al., 2010; Schwieter & Sunderman, 2008).",1
17,52,"Hence, while the study of Declerck et al. (2015a) indicates that language control could already initiate at the concept level, little to no further research has been conducted in this field. This demonstrates the pressing need for additional research into the role of the concept level during language control.",1
17,53,"Even though many models assume that language control occurs, at least partially, at the lemma level, very few studies have investigated this. Some evidence for the involvement of lemmas during language control comes from studies that investigated language switching with sentences instead of single words, since each lemma includes the syntactic information of the word, and sentence processing is assumed to occur at the lemma level (for a review on bilingual models of sentence processing, see Hartsuiker & Pickering, 2008). Tarlowski, Wodniecka, et al. (2013), for example, asked Polish–English bilinguals to describe a scene on a picture, either in a present progressive or a present perfective phrase. Their results indicated that language switch costs can be obtained when switching between sentences (for a similar result within sentences, see Bultena, Dijkstra, & van Hell, 2015). Furthermore, asymmetrical switch costs were obtained with progressive phrases and symmetrical switch costs with perfective phrases. From the latter finding it appears that different aspects of sentence processing, and thus lemma processing, can have an impact on language switching.",1
17,54,"Declerck and Philipp (2015a) also found evidence for the influence of sentence processing on language switching. Their results indicated no switch costs with the SBLS paradigm when German–English bilinguals produced words in sentences that were syntactically correct when translating it word-to-word to the other language (for other studies that found no switch costs with sentences, see also Dussias, 2003; Gullifer, Kroll, & Dussias, 2013; Ibá?ez, Macizo, & Bajo, 2010; Zhang et al., 2014). On the other hand, substantial switch costs were still observed in scrambled sentences or sentences that were not syntactically correct when translating it word-to-word. These results further support the influence of syntactic sentence information on language switching.",1
17,55,"The earliest studies that manipulated characteristics of phonology in language switching tasks were studies that investigated cognate status.2 Although studies exploring the role of cognates on switch costs found an effect, a discrepancy in the switch cost pattern has been observed between studies that implemented cognates or noncognates in individual blocks (e.g., Declerck et al., 2012) compared against studies that used cognates and non-cognates in the same block (e.g., Christoffels et al., 2007; Filippi et al., 2014; Thomas & Allport, 2000). In the former, switch costs are smaller with cognates, whereas in the latter switch costs are larger for cognates relative to noncognates.",1
17,56,"Taken together, while the present findings are encouraging, due to the small amount of studies it is hard to come to any final conclusions. More research in this theoretically important field of language control should help our understanding of the role of lemmas.",1
17,57,"Interestingly, Broersma (2011) found that switch trial interference was smaller after cognates than after non-cognates. This would explain the smaller switch costs with cognates than noncognates when they are both implemented in separate blocks (i.e., Declerck et al., 2012) as a persisting effect on the phonological level (e.g., persisting activation/inhibition). The larger switch costs when cognates and noncognates are presented in the same blocks could then be explained with withintrial control processes.",1
17,58,"Whereas the previous studies mainly manipulated phonological features within trials, Declerck and Philipp (2015b) investigated the effect of phonological overlap across trials. More specifically, words with the same first two phonemes as the word in the previous trial were compared against words that did not phonologically overlap with the word on the previous trial. The results showed that phonological overlap changed the switch cost asymmetry between L1 and L2. Hence, this study indicates that due to a phonological manipulation, persisting control effects were affected in language switching, providing additional support to the idea that phonological processing has an effect on language control.",1
17,59,"Yet, not all language switching studies find a clear effect by manipulating phonological characteristics. Declerck et al. (2013) found no switch cost difference between words that consisted solely of phonemes that occur in both languages (e.g., Bein, meaning Bleg^ in German) and words that contained language-specific phonemes (e.g., Katze, meaning Bcat^ in German). This study seems to indicate that differences in phonological aspects of the stimulus words (i.e., language-specific vs. language-unspecific phonemes) do not always have an effect on language control in language switching. It should be noted, however, that there was a numerical difference due to language-specific phonemes in L2 trials: The data showed larger L2 switch costs for words with language-specific phonemes (switch costs: 80 ms), relative to words without language-specific phonemes (switch costs: 48 ms). Hence, even in this study it appears that there is some indication for a role of phonology during language control",1
17,60,"There are several interpretations of these phonological studies: Some assume an indirect effect of the phonological level on language control at the lemma level (Declerck & Philipp, 2015b; Goldrick et al., 2014), whereas others assume that language control, next to the lemma level, also occurs at the phonology level (Olson, 2013). Goldrick et al. (2014) proposed that the difference in lemma activation, caused by language control in favor of the target language, spills over into the respective phonemes and thus allows for differences in phonology. Declerck and Philipp (2015), on the other hand, argued that there could be phonological feedback loops from the phonological representations back to the lemmas (see also, e.g., Bernolet, Hartsuiker, & Pickering, 2012; Costa, Roelstrate, & Hartsuiker, 2006; Dell, 1988). This would entail that the activation of lemmas would be influenced by phonology. In turn, language control at the lemma level would be influenced by phonology.",1
17,61,"Despite the discussion as to whether the phonological level plays a direct or indirect role in language control, the consensus seems to be that the phonology level plays an important role during language control. Further research into this area of language switching should help us narrow down the exact role.",1
13,1,"In 1998, I had the privilege of introducing Claire Schmais as she delivered the Marian Chace Foundation lecture. I emphasized in my introduction how essential she was to the world of dance therapy and how her absence would be almost incomprehensible (White as quoted in Schmais, 1999, p. 5). Sadly, our field now has to continue without her invaluable foresight, brilliance, and determination. We now have to rely on her significant scholarship and the impactful training she provided to her students, as well as the lived experiences shared by her peers.",0
13,2,"During the 22nd annual conference of the American Dance Therapy Association (ADTA), a group of presenters including Sharon Chaiklin, Beth Kalish, Claire Schmais, and myself shared papers on how our personal lives influenced our work as dance therapists (ADTA Monograph, 1987). Despite having only one conference call to organize the format, we did not reveal our individual plans to each other beforehand, resulting in a panel discussion that was particularly engaging and attentive. As co-founders of the ADTA who had worked together for 23 years, we were familiar with each other's personal backgrounds, but hearing our candid personal reflections on our experiences and emotions was an unusual and powerful experience.",0
13,3,"The panel presentation had Claire's own words, which beautifully described her and her motivations. I have extensively quoted her words as they were profound. For more than fifty years, Claire and I have been partners, and her intellectual and linguistic abilities have earned my respect and admiration. In these circumstances, sharing Claire's self-portrait seems the best thing to do. Revisiting her talk has been a source of joy for me as it reintroduced me to Claire's voice and her poignant description of herself.",0
13,4,"Claire's speech shed light on her exceptional traits such as her resilience, motivation, dedication to society, foresightedness, and influential persona. I have categorized her self-portrayal in each passage mentioned.",0
16,2,"The study revealed that the ECBI Intensity and Problem Scales have a one-dimensional structure in both samples, along with good internal consistency, test-retest reliability in the community sample, and excellent convergent and divergent validity. The ECBI Intensity Scale proved to be an excellent discriminant measure for differential diagnosis between various symptomatic conditions including ADHD, ODD, CD, and no diagnosis. The findings confirmed that the ECBI is a reliable measure of disruptive behavior in the Dutch population, though suggestions were made regarding how best to use the scale for research and screening purposes, along with additional research into the validity of the test for cross-cultural use.",0
16,3,"Intervening early is crucial to lessen the chances of severe disruptive conduct in adolescence and adulthood, as cited by Aos et al. (2004) and Heckman (2006). The most effective means of therapy for young kids and their parents are psychosocial treatments, according to Comer et al. (2013) and Eyberg et al. (2008). However, a reliable early screening process for behavioral issues in children is prerequisite to give such therapy. The most commonly used and efficient way of screening behavioral problems in young children is through parent rating scales (Funderburk et al. 2003).",0
16,4,"The ECBI, a parent rating scale designed to measure disruptive behavior in children aged 2 to 16, is widely used for early screening in clinical and research settings. Its strengths include sensitivity in measuring the effect of treatment on such behaviors, its brevity and ease of completion, and its utilization of two scales to assess disruptive child behavior. The ECBI is particularly suitable for screening in less educated families and less demanding than more comprehensive instruments.",0
16,5,"The ECBI is utilized in various locations around the world such as the United States (US), Europe, Japan, South Korea, and China, with translations available in multiple languages. The ECBI's reliability and validity have been supported by over 20 studies conducted across different cultures and countries. High internal consistency has been demonstrated in different sociodemographic subgroups through the two scales with alphas greater than .90. The ECBI has good retest reliability with evidence suggesting a correlation coefficient of .75 over a 10 month period. Normative data from community samples indicate that the mean scores on the ECBI are lower in Northern European countries including Sweden and Norway compared to the US as shown through studies conducted by Colvin et al. in 1999 and further explored by Axberg et al. in 2008 and Reedtz et al. in 2008.",0
16,6,"There are indications that the ECBI Intensity Scale is strongly associated with other widely used questionnaires that evaluate child behavior issues, such as CBCL (Achenbach and Rescorla 2000) and Strengths and Difficulties Questionnaire (SDQ; Goodman 1997), demonstrating satisfactory construct validity. In a non-clinical sample of Swedish children aged between 3 and 10 years, the correlation between the ECBI Intensity Scale and the total difficulties scale of the SDQ was found to be 0.68 (Axberg et al. 2008). In a clinically referred sample of US children between the ages of 4 and 16 years, the correlation between the ECBI Intensity Scale and the CBCL Externalizing Behavior scale was 0.75 (Boggs et al. 1990). As expected, the correlations with scales that measure externalizing behavior issues were higher than the correlations with scales that measure internalizing behavior problems (Axberg et al. 2008; Butler 2011). In terms of the discriminative validity of the ECBI, a clinically referred US sample studied by Weis and others (2005) found that the Intensity Scale was able to differentiate between groups of children with no significant externalizing problems, kids with inattentive and oppositional behavior symptoms, and children with more severe behavioral issues.",0
16,7,"Although the ECBI has a wide usage and strong validity evidence across countries, there is no evidence of its psychometric properties in the Netherlands and other European countries. Thus, it is crucial to have the knowledge of its psychometric properties for screening and treatment evaluations, especially in a Dutch community and clinical population. The present study aims to assess the psychometric qualities of the ECBI scales, including internal consistency, test-retest reliability, reproducibility, convergent, divergent, and discriminative validity, in both community and clinical samples. As the Intensity and Problem Scales of the ECBI have shown good psychometric properties in international studies, we expected similar results in this study.",0
16,8,"After initial findings supported the use of the ECBI as a three-factor measure for child behavioral problems, subsequent research by Gross et al. in 2007 provided backing for its validity as a one-dimensional measure. However, Butler's 2011 study in a community sample of low-income families from diverse cultural backgrounds and ethnicities failed to reproduce the three-factor structure and suggested that these factors were unsuitable for screening or treatment outcome research.",0
16,9,"Previous studies have utilized factor analysis to determine the factor structure of the ECBI. However, this approach is heavily reliant on the sample used, which can result in varying outcomes. While the three-factor structure of the ECBI has not been utilized in treatment outcome research, there remains a preference for the use of the ECBI as a single-dimensional measure of child disruptive behavior. Nonetheless, further exploration with a larger sample is necessary to better understand the optimal use of the ECBI Intensity and Problem Scales. Modern scale validation methods, such as Rasch analysis or Item response theory (IRT) analysis, that produce less sample-dependent outcomes could be used in this research.",0
16,10,"The study's second objective was to utilize modern test analysis techniques to verify the one-dimensional structure of the ECBI scales, thereby supplying additional insight into the ECBI's dimensional structure.",0
16,11,"The present study comprised of two groups of participants, namely a community sample (n=326) and a clinically referred sample (n=197). Prior to inclusion in the study, each individual was provided with informed consent and their agreement was obtained.",0
16,12,"The examination of the non-responsive group from the test-retest durability assessment revealed that there was reduced participation from the parents of children with a non-western heritage during the 6-month tracking (蠂2 (1)=9.19, p).",0
16,13,"A total of 326 parents, mostly mothers (86.8%), with children aged 2 to 8 years (M=5.54, SD=1.40) completed the ECBI. Of the children, 165 were boys and 161 were girls. Ethnic backgrounds were classified using Statistics Netherlands' (2013) criteria, resulting in three categories - 90.8% of the children were classified as Dutch, 4.9% as other western, and 4.3% as non-western. Parental education was divided into three categories: low (no or primary education), middle (secondary education), and high (higher academic education), based on Statistics Netherlands' (2014) classification.",0
16,21,"The ECBI scales showed high internal consistency in both the community sample (COS; IS & PS; 伪=.93) and the clinical sample (CLS; IS; 伪=.93, PS; 伪=.91). Father reports in the clinical sample were almost equal (IS; 伪=.93, PS; 伪=.92), and corrected item-total correlations were similar in both samples. The coefficients ranged from 0.09 to 0.73 for the ECBI Intensity and Problem Scales, with the median scores ranging from 0.46 to 0.55. Overall, the item-total correlation was satisfactory.",0
16,22,"The factor loadings of the ECBI Intensity and Problem Scale items on the first dominant factor were satisfactory and varied from 0.09 (item 36, Wets the bed) to 0.76 (item 10, Acts defiant when told to do something). The median factor loading scores were between 0.50 (CLS-PS) and 0.59 (CLS-IS). However, the factor loadings for item 36 (Wets the bed) were low in both samples.",0
16,23,"The mean scores for the ECBI Intensity and Problem Scales for both samples, categorized by age, sex, ethnicity, and informant's gender and educational level, are presented in Table 2. The subgroup analysis showed significant differences between boys and girls in the ECBI Intensity Scale in the community sample, with boys scoring higher (t (324)=2.32, p=.02), and in the ECBI Problem Scale in the Clinical Sample (t (175)=2.50, p=.01), with a small effect size (COS; d=.26, CLS; d=.38). Moreover, in the clinical sample, child ethnicity had a significant effect on the mother ECBI Intensity Scale (F=(2165) 10.88, p<.01), as revealed by one-way ANOVA.",0
16,24,"There were weaker correlations between measures completed by fathers compared to mothers across all scales, with mothers reporting more similar behavior problems on the ECBI and SDQ. As expected, the externalizing behavior SDQ scales had higher correlations than the Emotional Symptoms Scale and Peer Problems Scale. Additionally, the ECBI scales (especially the IS) had negative correlations with the SDQ Prosocial Behavior Scale.",0
16,25,"The aim of this study was to analyze the psychometric attributes of the ECBI in Dutch kids. The study looked at dimensionality, internal consistency, test-retest reliability, convergent, divergent, and discriminative validity. The results revealed that the ECBI possesses good psychometric characteristics in the Netherlands. These results align with our hypotheses and earlier international studies.",0
16,26,"The ECBI Intensity and Problem Scales showed good convergent and divergent validity with the SDQ in a clinical sample, which is consistent with previous studies. The strong correlations between the two scales and the similarity of patterns across different informants suggest the possibility of combining them into a single scale. However, Eyberg (1992) emphasized the importance of distinguishing between the two scales as they capture related but distinct dimensions of disruptive behavior in children. Parental perceptions may be the underlying construct of the separate scales, and they could be particularly useful in understanding parental tolerance.",0
16,27,"Parents who score low on Intensity but high on the Problem Scale may be experiencing high levels of parenting stress and intolerance towards their child's behavior. Conversely, those with a high Intensity score but a low Problem score may have a high tolerance for their child's behavior or may be hesitant to acknowledge any behavioral problems. While the ECBI scales are useful for understanding parental perceptions, future research should focus on the effectiveness of utilizing both scales in treatment and screening. It is recommended to use additional measures like observational methods and the Parenting Stress Index to assess child behavior problems and parental distress.",0
16,28,"The findings imply that weighing items during screening may be useful. For instance, if parents report frequent occurrences of a behavior on an item with significant weight, such as temper tantrums (item 13), the child is likely to score high on the ECBI Intensity Scale. Simply inquiring about the frequency of temper tantrums could help identify young children at risk for disruptive behavior issues and refer them for preventive treatment. This approach is innovative in using the ECBI scales and warrants more research on the item weighting system.",0
16,29,"The Dutch ECBI version may benefit from minor changes as per Axberg et al.'s (2008) suggestions. To reduce missing items, a checkbox for sibling items (25 & 27) would be useful where raters can signify if it applies to their child. Furthermore, low item statistics on item 36 (bedwetting) imply that an explanation of this item and a checkbox for applicability may be necessary. However, these changes could impact the total ECBI scores and require further consideration.",0
16,30,"The mean scores of the ECBI Intensity and Problem Scale for the overall community sample were considerably lower compared to the US norms discovered by Colvin and colleagues (1999) regarding normative data. This indicated that there was a significant difference between the two (IS; t (604)=5.83, p).",0
16,31,"The study has several limitations, including underrepresentation of ethnic minority groups in the community sample, an unknown response rate, and a high attrition rate. Therefore, there is a lack of information on the generalizability of the results. Further research should focus on psychometric properties, mean scores, response rates, and attrition prevention in multi-ethnic community samples. Additionally, the non-normal distributions of scores in the ECBI scales in the community sample were attributed to more frequent selection of low answer categories. Due to limited variation and small sample sizes, data from both samples were combined in the IRT analysis.",0
16,32,"The clinical diagnoses in the study were limited by the lack of a standardized procedure to assess significant symptoms of ADHD, ODD and CD in Dutch children. Therefore, the term classifications was used instead of diagnoses. However, caution should be exercised when interpreting the results regarding the classifications, as is common in Dutch clinical practice.",0
16,33,"The study's findings support the reliability of the ECBI as a measure of disruptive behavior in Dutch children. The ECBI Intensity and Problem Scales showed consistency and correlated with the established SDQ questionnaire. The Intensity Scale also distinguished between diagnostic groups in externalizing behavior. The ECBI's one-dimensional structure makes it practical for screening and intervention research, especially with the original scales. Incorporating weighted items could further enhance the efficacy of the ECBI, but this concept requires additional exploration.",0
16,34,The informed consent of all participants was obtained for the current study. The study followed the ethical standards of a medical ethics committee and was conducted in line with the 1964 Helsinki declaration and its subsequent amendments or equivalent ethical standards. Human participants were subject to all necessary procedures.,0
16,35,"The distribution of this article is authorized by the Creative Commons Attribution License, enabling usage, distribution, and reproduction in any medium, as long as there is proper attribution given to the original author(s) and the article's source.",0
17,1,"Language control, which restricts bilingual language processing to the target language, has been a key area of investigation in language switching. In this review, we delve into the mechanisms and loci of language control in language switching. We detail a range of empirical markers of language switching and their association with inhibition, an important mechanism of language control. Although some markers suggest inhibition during language switching, not all do. We then explore the potential loci of language control and the role of different processing stages. Studies indicate the involvement of several processing stages, suggesting a complex control mechanism or multiple loci of language control. Based on these findings, various established and new theoretical pathways are explored.",0
17,2,"Non-target language words can be accidentally activated and selected while processing language for bilinguals, leading to the need for language control to restrict production and decrease interference during comprehension. Language control is effective in preventing the selection of non-target language words during production, as shown in studies by Gollan et al. (2011) and Poulisse & Bongaerts (1994). A common task used to explore language control mechanisms is language switching. (Rephrased for clarity and coherence)",0
17,3,"The purpose of this review was to examine the mechanisms of language control in language switching and explore the significance of various processing stages (including concept level, lemma level, phonology, orthography, and outside language processing) in regulating language switching. Behavioral outcomes were the primary focus for both of these issues, and their correlation with language control models was evaluated. (For nonbehavioral language switching analyses, see Abutalebi & Green, 2007, 2008; Luk, Green, Abutalebi, & Grady, 2012).",0
17,4,"The cue language switching paradigm is widely used in language switching literature. This paradigm presents visual stimuli such as digits or pictures to indicate the concepts that must be named. Due to the implementation of multiple languages in language switching, visual language cues are utilized to signify the language in which the concepts need to be produced. As this procedure primarily applies to production studies, comprehension studies use written words instead. These inherently contain information about the necessary target language, negating the requirement for an additional, explicit visual cue.",0
17,5,"Meuter and Allport's (1999) influential study provides an instance of cued language switching where bilingual participants (speaking English, French, German, Italian, Portuguese, or Spanish) were given digits to identify the requested concept and colored rectangles (blue and yellow) to indicate the needed language. They were instructed to identify the digits based on the relevant language cue.",0
17,6,"Bilinguals had to either alternate between two languages or repeat the same language in consecutive trials. Studies by various researchers, including Meuter and Allport (1999), have consistently shown that there is a higher rate of errors and slower response times during language switching trials compared to repetition trials. This phenomenon is referred to as ""switch costs.""",0
17,12,"In order to perform sequence-based language switching (SBLS), bilinguals must memorize a sequence for language and concept. While no visual cues are needed for alternating language switching, SBLS presents visual language cues and stimuli. Bilinguals must follow an alternating language sequence without visual cues, but concepts must follow a specific sequential order. In response, bilinguals hear an auditory response-signal with no visual indication of when to respond. SBLS shows switch and mixing costs despite the predictability of upcoming responses.",0
17,13,"It can be inferred from various paradigms that switching languages typically incurs a performance cost, regardless of whether language cues or stimuli are used. This is evidenced by switch costs and/or mixing costs, which are observed in all types of experimental language switching, except for voluntary language switching under specific conditions. Along with this consistency, each paradigm has its own strengths and weaknesses.",0
17,14,"Research on language switching in language production tasks has provided a significant amount of information for the cued language switching paradigm. Although comprehension tasks usually do not implement cues, researchers have gained considerable knowledge about the effects and processes involved in cued language switching.",0
17,15,"The cued language switching paradigm offers another benefit in the form of altered time-based intervals, including the cue-to-stimulus interval that reflects active language preparation and the response-to-cue interval that reflects passive decay (Kiesel et al., 2010). These intervals cannot be as easily manipulated in other language switching paradigms.",0
17,16,"The paradigm of alternating language switching permits preplanning of language, since the bilingual person knows when they will switch to another language, which is akin to preplanning during natural language production. Therefore, they can prepare for the next language by following the predictable sequence.",0
17,17,"Even more similar to switching between languages in everyday conversation is the voluntary language switching paradigm. This approach doesn't depend on outside language prompts but on internal ones, which means that bilingual speakers can plan in advance which language to use. Furthermore, voluntary language switching allows for the collection of data on when and how frequently bilingual speakers switch between languages, providing additional perspectives into the intricacies of language control during switching.",0
17,18,"The SBLS paradigm enables sequential predictability of language control, which applies to language predictability in alternating and voluntary language switching paradigms, but in SBLS, it covers the entire word. One of the benefits of this paradigm is the broad scope of word categories that can be analyzed due to the memory-based concept sequence that allows for a wide range of words. However, other paradigms can also investigate different word categories if they utilize written words.",0
17,19,"There is a wide range of language switching paradigms available, each with its own pros and cons, that can be utilized to investigate language control in language switching. However, all of these paradigms share the same objective of enabling researchers to examine the mechanics of language control and identify the key factors involved in language switching.",0
17,20,"The inhibitory control model (ICM) by Green (1998) is an influential theory regarding language control. According to this model, switch costs stem from persisting inhibition. When a trial (n1) requires the production of a certain language, the non-target language is inhibited. In case the previously inhibited language is required for the next trial (n), the inhibition persists and needs to be overcome. However, if the same language is produced on trial n-1 and trial n, there is no need to overcome inhibition as in repetition trials. As per Green, the inhibitory process is reactive and more activation leads to more inhibition. Hence, it is harder to switch between languages due to the presence of persisting inhibition in switch trials.",0
17,21,"Whereas the ICM has a production-based focus, the BIA model and its variants concentrate on comprehension-based language control. Switch costs are explained by the bottom-up activation of word representations to language nodes, which are mental language representations, and top-down inhibition of the irrelevant and competing language node. Switch trials activate a different language node, leading to interference with processing, while repetition trials use the same language node, allowing processing to occur without language interference. Therefore, switch costs are expected.",0
17,22,"There are alternative language control models that do not depend on inhibition, which include studies by Costa, Miozzo, and Caramazza (1999), Finkbeiner et al. (2006), La Heij (2005), and Roelofs (1998). La Heij contends that a language cue heightens activation in the target language representation, and therefore inhibition is not necessary.",0
17,23,"In contrast to the former models, which suggest that there is no inhibition involved in language switching, various studies have proposed inhibition as a key explanation for different language switching measures. These include asymmetrical switch costs, n-2 language repetition costs, and reversed language proficiency in mixed language blocks. The next sections will further explore these markers and their connection to inhibition.",0
15,1,"In the last two decades, corticomuscular synchronization during isometric compensation of static forces has been extensively studied. Beta-range (15-30 Hz) corticomuscular coherence (CMC) has been reported between motor cortical neurons and muscles in monkeys [1-4] and between sensorimotor cortex and muscle activity (EMG) in humans [5-14].",1
15,2,"Several investigations focused on the mechanisms by which cortex drives the muscles under dynamic conditions. It was shown that gamma-range (above 30 Hz) CMC reflected effective corticospinal interactions and different gamma-subranges were associated with various motor tasks [15-19]. For instance, Schoffelen et al [15] found that the subjects’ readiness to respond in a simple reaction-time task was closely correlated with the strength of one gamma-range (40-70 Hz) CMC between motor cortex and EMG activity. In addition, Brown and colleagues [20] showed that, while weak static forces were accompanied by beta-range CMC, gamma-range (35-60 Hz) CMC occurred mainly for submaximal and maximal forces. Significant ECoG-EMG coherence in the high gamma subrange (61-100 Hz) was also reported during phasic movements [21]. For a visuomotor isometric compensation of a periodically modulated force at 4% MVC we found a lower (30-45 Hz) CMC gammasubrange [18,22,23]. We suggested that this low gammasubrange reflects the rapid integration of proprioceptive, visual and cognitive (preparatory attention) information required to produce the appropriate motor command. Since we had found that this gamma-range CMC is not modulated by the amplitude of the modulated force [23] we wondered whether it would be modulated by various frequencies.",1
15,3,"We addressed this question investigating the CMC, cortical motor spectral power, as well as motor performance during a visuomotor task, where subjects compensated isometrically a periodically modulated force at 8% MVC with three frequencies (0.6, 1.0 and 1.6 Hz).",1
15,4,"We tested the following predictions: First, based on our earlier results showing a shift of the CMC from beta- to gamma-range during compensation of static and periodically modulated force respectively [18,23,24], we predicted that with increase in frequency modulation the CMC will be shifted towards higher frequencies in order to effectively integrate sensorimotor information.",1
15,5,"Second, recent studies on corticospinal interaction during rhythmic hand movements have reported that increased beta-range CMC was accompanied by a CMC peak at the frequency of the movement or of the periodic muscle contraction [25,26]. Based on these results, we predicted CMC peaks at the frequencies of modulated force. We also speculated that these low-frequency CMC peaks should be stronger for more difficult tasks and hence, for higher frequencies.",1
15,6,"We found that beta- and gamma-range CMC and cortical motor spectral power were not modulated by the various frequencies of the modulated force. However, a sharp bilateral CMC peak at 1.6 Hz was observed, but only in the five gamma-range CMC subjects. Our findings suggest that the tested frequency of force modulation have no effect on the beta- and gamma-range CMC during isometric compensation for modulated forces at 8% MVC. In addition, they support that the frequency range of CMC depends on a multiplicity of factors, i.e. task parameters, inter-individual differences and possibly the behavioral strategy applied by each individual.",1
15,7,"Eight healthy right-handed subjects (female, mean age 28 ± 10 years) without any history of neurological disease participated in the study. Handedness was tested according to the Oldfield questionnaire [27]. Three of the subjects had already participated in similar experiments in our lab. All subjects participated according to the declaration of Helsinki, with informed consent and the approval of the local ethics committee.",1
15,8,"During the experimental session, the subject sat in an electrically shielded, dimly lit room. The right arm was supported by a splint and the subject was instructed to place the hand over a sphere and the right index finger in the ring of a home-made manipulandum (see Figure 1B).",1
15,9,"The manipulandum was designed for applying vertical forces on the finger at the level of the metacarpophalangeal joint. A computer-controlled tooth belt drive produced a variable force on the ring. The subject had to compensate the force generated by the manipulandum isometrically to maintain the ring in its initial position (see Figure 1B). Visual feedback (see Figure 1C) about the position of the ring was provided via a 19’’ monitor placed 100 cm in front of the subject and displaying two concentric circles. The green outer circle was located in the centre of the screen and represented the ring’s reference position while the white inner circle moved corresponding to the ring’s actual position. The subject had to maintain the small white circle inside the green one, so that when a given force was applied to the ring the subject had to compensate it by generating force in the opposite direction (here flexion). The sensitivity of the visual feedback with respect to the finger position corresponded to 2 mm on the screen for 1 mm ring displacement.",1
15,10,"The target force had four different phases (Figure 1D): a ramp phase (rising cosine function) which ensures a smooth start of the generated force. In all experimental conditions, the force level, or ramp amplitude, was 8% MVC. The 1 s ramp phase was followed by a 3 s-period of static force (T0) that gave time to stabilize the force to the “0” position (8% MVC). After the static period, the sinusoidally modulated force period with 8% MVC peak-to-peak amplitude and lasting 15 s was followed by downward ramp phase (again cosine function) to ensure a smooth end of the generated force.",1
15,11,"Three different experimental conditions were investigated in a given recording session (Figure 1D): 
• 0.6 Hz condition (W1): The frequency of the force modulation was 0.6 Hz (Figure 1D, left panel). 
• 1 Hz condition (W2): the frequency of the force modulation was 1 Hz (Figure 1D, middle panel). 
• 1.6 Hz condition (W3): The frequency of the force modulation was 1.6 Hz (Figure 1D, right panel).",1
15,12,"The three frequencies W1, W2, and W3 were carefully selected so that they were equidistant on a logarithmic scale, holding the following relations: W2 = 5/3 × W1, and W3 = 8/5 × W2 where (5/3 = 1.67) is approximately equal to (8/5 = 1.6). Therefore, W2 is approximately the geometric mean of W1 and W3. This selection is in line with the notion that signal discrimination in humans is usually following logarithmic rules (see [28]). Besides, any single stimulus frequency would not overlap with the frequency spectrum of another stimulus frequency including its harmonics in order to reduce unwanted crosstalk [29].",1
15,13,"Prior to the experiment, we recorded rest EEG for 5 minutes while subjects were attending at the small white circle and their right hand was resting over the sphere with the right index finger in the ring of the manipulandum. During this rest period no force was applied by the manipulandum, so that the index finger remained static in its initial position. After that the force corresponding to the individual MVC was measured. An experimental session consisted of 5 different recording series, while each series included 18 trials. The 3 experimental conditions W1, W2 and W3 were presented in a pseudo-randomized fashion within the 18 trials, so that each frequency appeared 6 times within one recording series. Therefore, a total of 30 trials were recorded in each subject for each of the three frequency conditions. To avoid muscle fatigue, rest intervals of 7 to 12 s were included between the trials and approx. 5 min between the series.",1
15,14,"To optimize performance and to avoid attentional variation across trials, the subjects were requested to concentrate on the temporal structure of the applied force profile and to tune the isometric contraction of their finger muscles to the identified force frequency. After each trial, they had to verbally report the frequency of the force modulation as ‘slow’, ‘middle’ and ‘fast’, corresponding to the frequencies 0.6 Hz (W1), 1 Hz (W2), and 1.6 Hz (W3) respectively.",1
15,15,"The subjects were instructed to avoid any other movements and to fix their gaze on the concentric circles displayed on the screen. Before the onset of the recordings, subjects performed some trials to get familiarized with the task.",1
15,16,"Electrical potentials (bandpass 0-200 Hz, sampling rate 1000 Hz) were recorded from 58 scalp positions according to the international 10-10 system (Synamp 2, NeuroScan, El Paso, TX, USA) referenced to Cz (Figure 1A) with ground at FzA. Electrode impedances were kept under 5 kOhm. The electrooculogram (EOG, same bandpass and sampling rate as for EEG) was recorded to exclude trials contaminated with eye movements from further analysis. Electromyographic activity (EMG, bandpass 0-200 Hz; sampling rate 1000 Hz) was recorded with surface electrodes using a belly-tendon montage from three muscles: the pars indicis of the right flexor digitorum superficialis (FLE), prime mover of the index finger flexion, the right first dorsal interosseus (FDI), and the right extensor digitorum communis (EXT).",1
15,17,"In each condition the recorded finger position was first cut into 30 epochs of 24 s each starting from 2 s before the onset of the trial (Figure 1E). Then for each participant, the temporal profile of the mean finger position was obtained by averaging the 30 epochs, and the grand average of the finger position was computed across all participants. Based on the grand average of the finger position, three periods (T0, T1 and T2) were identified (Figure 1E). The 3 s static force period was named T0. T1 corresponded to the period during which the finger position showed large transient fluctuations. T2 corresponded to the period in which the finger position reached a steady state and remained stable until the end of the force modulation.",1
15,18,"Only data from period T1 and T2 was included in the analysis. In each trial, the data recorded during the 15 s sinusoidal force modulation (Figure 1E) was first separated in two data sets corresponding to periods T1 (3 s) and T2 (12 s). Then, for both periods (T1 and T2), data was further cut into segments with an overlap of 50%. Segments had duration of 1 s, therefore allowing a frequency resolution of 1 Hz for further spectral analysis. Artifact rejection was visually performed off-line trial-by-trial to exclude segments contaminated with eye movements. The EEG signal was then transformed into the reference-free current source density distribution (CSD) which reflects the underlying cortical activity [30]. The CSD algorithm was estimated using the spherical spline interpolation method [31] implemented in the commercial software ‘BrainVision’ 1.05 (München, Germany). For each subject, 100 artifact-free segments were obtained from the period T1, while 400 segments were obtained for the period T2.",1
15,19,"EMG signals were rectified, as it is known that fullwave rectification, providing the temporal pattern of grouped firing motor units [32], is an appropriate procedure for power and coherence analysis [33]. The discrete 1000 points Fourier transform was computed for each segment.",1
15,20,"Figure 1D shows the target force profiles for the three conditions W1, W2, and W3. In all three conditions fluctuations (~ 3 mm) of the ring position during T0, i.e. the static force following the force ramp, were observed (Figure 1E). The force ramp was compensated by all subjects before the end of the period T0, so that the finger came back to the ‘0’ position before the onset of the force modulation.",1
15,21,"For W1, W2 and W3 the grand-averages of the finger position during the 30 trials in all participants are shown in Figure 1E. The frequency of the oscillations corresponded to the target frequency in all three conditions. Note that finger oscillations started with larger amplitudes during the initial 3 s (period T1) but stabilized at slightly lower amplitudes during the period T2, where a steady-state oscillatory performance is observed around the ‘0’ position with deviations of maximally 1.5 mm in both directions.",1
15,22,"Figure 2A shows the mean DR values for the three conditions W1, W2 and W3 for all subjects. One can see that subjects successfully identified the frequency of the force (as slow, middle or fast) in more than 90% of the 30 trials. In addition, the DR showed a tendency to increase from ~90% to ~95% with higher frequency of force modulation, so that the highest detection rate (DR) was observed for W3. This tendency was present in most of the subjects. This suggests that subjects were more aware of the temporal force profile when the force modulation rate increased. However, the Friedman test for the DR values did not show any significant difference among the three conditions.",1
15,23,"The statistical comparison of the performance errors (PE) in the two periods T1 and T2 revealed a highly significant difference (F(1,7) = 95, p < 0.001), reinforcing the fact that fluctuations in finger position were higher during the transition period T1, as can be seen in the grand-average of the finger position (Figure 1E). In addition, PE significantly increased at higher frequencies (F(1,7) = 10.7, p < 0.01, see Figure 2B). The analysis of the polynomial contrast showed that the increase of PE for higher frequencies was due to a linear effect (p = 0.02), the quadratic part being not significant.",1
15,24,"The maximum EEG-EMG coherences were observed over the contralateral sensorimotor cortex (C3 or C1). Individual EEG-EMG coherence curves of the eight subjects for the 0.6 Hz, 1 Hz and 1.6 Hz conditions during the periods T1 and T2 are shown in Figure 3 and Figure 4 respectively.",1
15,25,"During the transitory period T1 the CMC spectra contained several random-like sharp peaks distributed in the 15-45 Hz range (Figure 3). In period T2, when motor performance reached a rather stable state, the CMC spectra showed consistent broadband coherences within the beta- (15-32) and gamma (25-45 Hz) range (Figure 4). The three-way ANOVA revealed significant main effect of the factor Period with significantly higher CMC in T2 (F(1,7) = 21, p < 0.001).",1
15,26,"As seen in Figure 4, five of the subjects have broad-band CMC mostly in gamma-range (25-45 Hz) while three subjects had it in beta-range (15-32 Hz), as supported by a significant main effect of factor Subject for the CoG (F(1,7) = 9, p < 0.01). The subjects were accordingly classified in more beta- and more gamma-group. Note that although the CMC data fits fairly well to this clear-cut classification, there are subjects with peaks in both beta and gamma-range. For example, in Figure 4 the gammasubjects S4, S5 and S6 show lower peaks in beta-range. The classification in beta- and gamma-groups was meant to capture the major differences of the CMC pattern.",1
15,27,"No significant effect of the main factor Frequency was found. Thus, the outcome of the statistical analysis reveals that neither the CMC amplitude nor the CoGs are influenced by the frequency of the modulated force.",1
15,28,"We also looked for significant CMC values within the 0 - 5 Hz range, peaking at the frequencies of force modulation (W1, W2 or W3) or their harmonics. All five gamma subjects had a low-frequency CMC peak at 2 Hz, that matches the frequency modulation W3 (1.6 Hz), according to the 1 Hz spectral resolution. This low-frequency CMC peak was consistently observed in the 1.6 Hz condition only (Figure 4). This CMC peak at 2 Hz in W3 condition was observed not only over the contralateral, but also over the ipsilateral sensorimotor cortex, as displayed in the topographic maps of one individual and of the grand average in Figure 5. Significant low-frequency CMC peaks at 2 Hz were observed only in electrodes above sensorimotor areas. None of the three beta-group subjects had such a peak (see Figure 4, left column).",1
15,29,"With respect to the cortical motor SP amplitude, the ANOVA did not reveal any significant main effect for the three tested factors or their interactions. Subjects showing increased CMC in beta- or gamma-range did not show any corresponding increase in spectral power in the same frequency range.",1
15,30,The cortical spectral power during rest did not show any differences between beta- and gamma-range subjects in terms of beta and gamma power.,1
15,31,"The present study was designed to investigate the corticospinal interactions during isometric compensation for force modulated at three different frequencies (0.6, 1.0, 1.6 Hz). A stable CMC and performance only occurred after a transitory phase in which the force had to be adjusted to the modulation frequency. During the transitory phase (period T1), the CMC spectra contained several random-like sharp peaks distributed in the 15-45 Hz range. This pattern may arise due to a transitory regime of corticospinal circuits functioning during the first 3 s. During the stationary phase (Period T2), CMC occurred in five of the eighth subjects in gamma-range and in three of them in beta-range. The findings from the study show the presence of significant broad-band gamma-range CMC in five of the eight subjects and betarange in three of them. Neither the gamma-range CMC, nor the beta-range CMC were modified by the various force modulation frequencies. In addition, a sharp CMC peak at 2 Hz was observed only during the highest frequency of force modulation (1.6 Hz) suggesting that corticospinal circuits resonating at the force frequency also play an important role in our isometric force compensation task.",1
15,32,We were specifically interested in studying CMC during high-precision slow-paced natural movements which typically occur within the 0-2 Hz frequency range. The findings show that increasing the force modulation frequency within this range did not induce any changes in amplitude or frequency range of the beta- and gammarange CMC.,1
15,33,"Any interpretation of this result should take into consideration the following: Corticospinal oscillations may be actually modulated by the frequency of the force, but this effect could not be manifested in our experimental design due to the specific frequency range of interest (0.6 - 1.6 Hz). This range was delimited by several constraints: First, the upper extreme of this range was set at 1.6 Hz to avoid muscular stiffness associated to higher frequencies, while the lower extreme was set above 0.5 Hz to facilitate the sensory perception of the dynamically modulated force. Second, CMC could be actually modulated by the force frequency even within this range of interest, but we cannot easily detect this effect with non-invasive recordings such as EEG or MEG due to spatiotemporal smearing of neighboring neural sources [35,36]. Therefore, intracortical recordings are needed to further clarify this issue. Within these constrains and limitations, our results rather align with the view that CMC is not modulated neither by the amplitude [23] nor the frequency of the modulated force.",1
15,34,"The finding of five gamma- and three beta-range subjects is difficult to interpret when considering that performance error (PE) and awareness of the modulation frequencies as reflected in the DR did not differ significantly between the two groups. Thus, difference in the CMC frequency range was not correlated to behavioral performance. Moreover, this finding was not related to differences in experience in similar experiments. These results suggest that there are intrinsic inter-individual differences in CMC, whereas subjects may equally perform the visuomotor task recruiting different functional corticospinal circuits resonating in the beta- or gamma range. We described this in a previous work of ours investigating the CMC during modulated forces at 8% MVC [17].",1
15,35,"However, in addition to the interindividual differences some other differences could be taken into consideration: The beta- and gamma-subjects may have differed in their strategy to compensate for the modulated forces. Subjects were requested to attend to the force modulations in order to recognize and report the frequency as slow, middle or fast. Nevertheless, subjects may have chosen: i) to encode precisely the temporal profile of the force and reproduce the “dynamic” pattern, generating a force in the opposite direction, or ii) to exert a “pseudo-static force” at the 8% MVC level, independently of the modulation frequency. The first, “dynamic”, strategy may elicit corticospinal oscillations in gamma-range to rapidly and continuously integrate proprioceptive, visual, and cognitive (preparatory attention) information and meet the demands of a dynamic environment [18]. The second, “pseudo-static”, strategy, even in the presence of a dynamic modulated force, is mainly relying on the static features of the force profile (the 8% MVC force level) and may lead to beta-range CMC, which is shown to be associated with rather rigid and stable regimes of corticospinal interactions [11,37,38].",1
15,36,"Our previous studies reported beta-range CMC during isometric compensation for low-level static forces at 4% MVC and predominantly gamma-range (30-45 Hz) CMC during dynamic force compensation [8,11,18,23]. However, in a recent investigation of the CMC during a visuomotor task where different levels of a modulated force (8, 16, 24% MVC) were applied [17], we also found broad-band beta-range CMC. Taken together, we conclude that beta-range CMC is not confined to or specific for low-level static forces only. Rather, the sensorimotor system may resort to either beta- or gamma-range CMC to generate effective corticospinal interaction when compensating for dynamic modulated forces. Both beta- and gamma-range CMC represent mechanisms for effective corticospinal interaction and can be selectively used to subserve different functions [8,15].",1
15,37,In her review article Tallon-Baudry [39] concludes that there is no one-to-one relationship between a frequency band and a single cognitive function. Our results demonstrate that there is no one-to-one relationship between frequency band and specific motor function either.,1
15,38,"For the highest force modulation frequency investigated (1.6 Hz) we found an additional sharp CMC peak at the same frequency. This low-frequency peak was observed only in the five gamma subjects. Similar CMC patterns over the sensorimotor area have been observed in recent studies of corticospinal interaction for tasks requiring to synchronize rhythmic foot movements or periodic isometric contraction of calf muscles to external periodic events [40]. In this work, a CMC peak at the frequency of the movements or muscle contraction was associated with corticospinal synchronization processes during dynamic motor output. Following the same line of reasoning, we interpret the co-existence of gamma-range CMC and of a CMC peak at 1.6 Hz (modulated force frequency) as support to our view of gamma-range CMC as reflecting effective corticospinal interaction. The fact that this CMC peak at 2 Hz was found only for the highest frequency tested (1.6 Hz) can be explained when considering that in this condition the performance errors were significantly larger, indicating an increase in task difficulty. In fact, previous work by Flowers [41] has shown that difficulties in performance of a tracking task increase for frequencies above 1.5 Hz.",1
15,39,"Nesting of high-frequency oscillations (gamma) into low-frequency ones (theta) is suggested to multiplex processes in the same location [39,45,46]. Lakatos et al. [47] hypothesized that slower rhythms provide windows, in which the high-frequency rhythms are activated. It is possible that in our study the nesting of the gammarange CMC into 1.6 Hz oscillations contributes to cope with the higher task demands in this condition.",1
15,40,"The detection rate (DR) was higher for higher force frequencies, with value above 95% for the 1.6 Hz modulation. This result is in line with studies showing that perceptual awareness of external periodic events increase with the rate of change of these events [48,49]. However, a relationship between awareness of force frequency and a CMC peak at the same frequency remains at best highly speculative, and should be addressed in experimental paradigms where level of awareness of the force profiles are explicitly manipulated.",1
15,41,Our findings suggest that the frequency of force modulation has no effect on the beta- and gamma-range CMC during isometric compensation for modulated forces at 8% MVC. The beta- and gamma-range CMC may be related to interindividual differences. The sharp CMC peak at 2 Hz during the highest frequency of force modulation (1.6 Hz) suggest that corticospinal circuits resonating at the force frequency also play an important role in isometric force compensation. Our results are a step towards further understanding of the global oscillatory processes and help to get new insights in the dynamics of neural systems [50].,1
17,1,"Hypoalbuminemia can be caused by various conditions, including nephrotic syndrome [1,2], heart failure [3], liver disease [4,5] and malnutrition [6]. Most cases of hypoalbuminemia among hospitalized patients are caused by acute and chronic inflammatory responses [7]. Moreover, a strong association has been reported between the serum albumin level and mortality [8]. The serum albumin level is an independent risk factor for all-cause mortality in older persons and an important prognostic indicator [9].",1
17,2,"From 1990, we have continued carrying out health screenings of the residents of X town (adult population: 7,389) in northern Kyushu, Japan, where the prevalence of hepatitis C virus (HCV) infection is the highest in the country and the mortality from liver cancer is about three times the national average [10-23]. The positive rates of antibodies to HCV (anti-HCV), HCV RNA and hepatitis B surface antigen (HBsAg) were, respectively, 23.6%, 17.9%, and 2.6% in 1990 [15]. We demonstrated extrahepatic manifestations as well as the natural course and carcinogenesis of HCV-infected persons in X town.",1
17,3,"There has been little discussion about hypoalbuminemia and mortality over the long term in residents of the area. In this study, we determined whether serum albumin levels impact on the life prognosis of the residents of X town after a follow-up period of 12 years.",1
17,4,"In 1990, 10% (739 people) of the 7,389 inhabitants were selected randomly and, as a result, 509 subjects participated in the study for examination of liver diseases accompanying HCV or hepatitis B virus (HBV) infections [15]. We studied 509 consecutive residents prospectively for 12 years. Of these 509 subjects, 69 had died and 55 had moved to other regions by May 31, 2002. Therefore, 385 of the original inhabitants investigated in 1990 continued to reside in X town in May 2002. Consequently, 454 residents, whose life and death could be confirmed between 1990 and 2002, were studied. The albumin levels were categorized into two groups, low (<4.0 g/L, group A) and normal (≥4.0 g/L, group B) and there were 25 subjects in group A and 429 in group B.",1
17,5,"In 1990, sera were provided by the 454 subjects for the following serological assays: albumin, serum aspartate aminotransferase (AST) and alanine aminotransferase (ALT). Sera were also examined for the presence or absence of markers of HCV and HBV infection. AntiHCV was measured by a chemiluminescent enzyme immunoassay (CLEIA) kit (Lumipulse II HCV, Fujirebio Inc., Tokyo, Japan). HCV RNA was detected in the sera using the Amplicor HCV test (Nippon Roche, Tokyo, Japan). HBsAg was assayed by a chemiluminescent immunoassay (CLIA) kit (Architect™, HBsAg QT, Dainabot Co. Ltd., Tokyo, Japan). Ultrasonographic examination of subjects with abnormalities in their liver function tests and who were positive for anti-HCV or HBsAg was performed in order to investigate the shape of the liver and lesions occupying the hepatic space.",1
17,6,"Obesity was defined as a body mass index (BMI) ≥ 25 kg/m 2 or greater. We also took a history of liver diseases, smoking, and alcohol consumption. We compared these factors between group A and group B. The total intake of alcohol was estimated on the basis of information about the consumption of beer, wine, whisky, Japanese sake, and shochu. In addition, the cumulative ethanol consumption up to 1990, expressed in kilograms, was calculated approximately by converting the alcohol intake in a serving of each type of alcoholic beverage into grams.",1
17,7,"Of the 509 inhabitants examined in 1990, 69 (34 men and 35 women; mean age at death, 76.6 years) had died by 2002. We compared the causes of death in group A and group B.",1
17,8,"All data are expressed as mean ± standard error. Differences between the two groups were analyzed using the Mann-Whitney U test, Wilcoxon’s test, and the Fisher’s exact test. Differences were judged significant for p < 0.05 (two-tailed). Adjusted odds ratios were calculated using logistic regression analysis. All statistical analyses were conducted using JMP Version 6 (SAS Institute, Cary, NC, USA). The level of statistical significance was defined as 0.05. Survival analysis was carried out using the Kaplan-Meier method.",1
17,9,"The details of the 454 subjects studied are shown in Table 1. We compared the characteristics of 25 subjects whose serum albumin was <4.0 g/L (group A) and 429 subjects whose serum albumin was ≥4.0 g/L (group B). The mean age in group A was 68.8 ± 14.5 years and there were 16 men and nine women. The mean age in group B was 51.9 ± 15.9 years and there were 180 men and 249 women. Being male (P < 0.05), elderly (P < 0.0001), having a history of liver diseases (P < 0.01), history of smoking (P < 0.05), abnormal AST level (P < 0.01), being positive for anti-HCV (P = 0.0001), positive for HCV RNA (P < 0.001), and occurrence of death (P < 0.00001) were significantly more common in group A than in group B (Table 1). Mortality was 68.0% in group A (17/25 cases, P < 0.00001 vs. group B) and 12.1% (52/429) in group B, as shown in Table 1 and Figure 1. No significant differences were observed between the two groups regarding BMI, alcohol consumption, ALT level, and positive rate of HBsAg.",1
17,10,"Individuals were stratified according to cumulative ethanol consumption by 1990: non-drinkers (227, 50.0%), <10 kilogram (62, 13.7%), 10-50 kilogram (37, 8.1%), 50-100 kilogram (21, 4.6%), and ≥100 kilogram (107, 23.6%).",1
17,11,"Table 2 shows causes of death for groups A and B. The numbers of deaths from malignant tumor were 9 (52.9%) in group A and 19 (36.5%) in group B. These fatal malignant tumors were hepatocellular carcinoma (HCC, six), gastric cancer (two) and prostate cancer (one) in group A and lung cancer (six), colon cancer (four), HCC (three), gastric cancer (two), esophageal cancer (one), leukemia (one), malignant lymphoma (one) and unknown (one) in group B. Mortality from HCC was 66.7% (6/9 cases, P = 0.01 vs. group B) in group A and 15.8% (3/19) in group B. No significant differences were observed between these two groups in terms of the numbers of death from malignant tumors other than HCC.",1
17,12,"No significant differences were observed between the two groups for mortality from cerebrovascular disease, cardiac disease, pneumonia, liver disease, diabetes mellitus, suicide, tuberculosis, a freak accident, feebleness of age, and others.",1
17,13,"According to multivariate analysis, five factors - 50 years or older, low albumin level (<4.0 g/L), abnormal AST level, history of smoking, and absence of alcohol consumption - were associated with death. The adjusted odds ratios for these five factors were 20.65, 10.79, 2.58, 2.24 and 2.08, respectively, and each was statistically significant (Table 3).",1
17,14,"Cumulative ethanol consumption of <10 kilogram or 10-50 kilogram played an important role in survival. The adjusted odds ratios compared to absence of alcohol consumption were 6.44 (95% confidence interval: 1.93-39.92), and 7.72 (95% confidence interval: 1.62138.46), respectively.",1
17,15,"Low serum albumin levels are an important predictor of morbidity and mortality [8,9] and correlate with an increased risk of morbidity and mortality in hospitalized patients. However, there has been little discussion about hypoalbuminemia and mortality of the residents of an area with an exceptionally high prevalence of HCV infection. In this study, we determined whether serum albumin levels affect the life prognosis of the residents of X town.",1
17,16,Our results indicate a strong association between hypoalbuminemia and mortality in this hyperendemic area of HCV infection in Japan. Residents with hypoalbuminemia had a mortality of 68.0%; dramatically higher than the rate of 12.1% among residents who had normal albumin levels. We previously reported that HCV infection and ALT value were associated with deaths due to HCC or liver cirrhosis in this X town [17]. We also showed that hypoalbuminemia was prognostic factor about all-cause mortality.,1
17,17,"It is estimated that ~170 million people worldwide are infected with HCV [24], some two million (1%) of whom reside in Japan [25]. HCV leads to serious consequences such as liver cirrhosis and HCC. Of the HCC cases in Japan, around 16% are caused by hepatitis B virus (HBV) infection and around 80% by HCV infection. The increase in the number of HCC patients due to HCV contributes to the increase in total deaths in Japan from HCC. This trend is expected to continue until 2015 [25].",1
17,18,"Albumin, produced only by the liver, is the major protein that circulates in the blood. Albumin consists of 585 amino acids, has a molecular weight of approximately 69 kDa and is the most abundant plasma protein, although 60% of the total albumin pool is in the interstitial space [26]. Albumin is essential for maintaining the oncotic pressure in the vascular system. A decrease in oncotic pressure due to a low albumin level allows fluid to leak from the interstitial spaces into the peritoneal cavity, producing ascites. Albumin is also very important in the transportation of various molecules, including bilirubin, free fatty acids, drugs, and hormones. Serum albumin is an abundant multifunctional non-glycosylated, negatively charged plasma protein, with ascribed ligand-binding and transport properties, antioxidant functions, and enzymatic activities [27].",1
17,19,"A low serum albumin concentration indicates poor liver function. Decreased serum albumin levels are not seen in acute liver failure because it takes several weeks of impaired albumin production until the serum albumin level drops. The most common reason for a low albumin is chronic liver failure caused by cirrhosis. The serum albumin concentration is usually normal in chronic liver disease, until cirrhosis and significant liver damage develops. In advanced liver disease, the serum albumin level may be less than 3.5 g/dl. The albumin level is clinically important as a predictive factor for patients with liver cirrhosis, because decreased serum albumin levels cause ascites and edema.",1
17,20,"Recent studies have demonstrated the efficacy of branched-chain amino acid (BCAA) supplementation in improving hypoalbuminemia in cirrhotic patients [28]. Kotho et al. investigated the correlation between albumin levels and the fat-free mass in cirrhotic patients [29]. They showed that exercise and proteinrich nutrition at the early stage of liver cirrhosis may be advisable for maintaining or increasing muscular volume. Nishiguchi et al reported that if cirrhotic patients were in the compensated stage at the entry but with lower BCAA tyrosine ratio (BTR), oral BCAA supplementation might be effective in maintaining serum albumin [30]. Stating appropriate nutritional interventions, such as supplementation of BCAA, in the early stage of cirrhosis may improve prognosis and maintain QOL. We also reported that the administration of BCAA supplement (Aminofeel ® ) increases serum albumin levels and serum zinc levels, and improves sensitivity to different tastes [31-33].",1
17,21,"In conclusion, we demonstrated that the serum albumin level is an independent risk factor for mortality from all causes and an important prognostic indicator in the residents of X town. In particular, improvement of hypoalbuminaemia as well as the eradication of HCV, such as by interferon therapy, should be considered for improvement of prognosis in this hyperendemic area of HCV infection in Japan.",1
18,17,"There is increasing evidence for the involvement of miRNAs in mammalian biology and breast cancer. For instance, the levels of MiR-206 have been found to be higher in ERalpha-negative MB-MDA-231 cells than in ERalpha-positive MCF-7 cells [12], and enforced expression of miR-125a or miR-125b leads to coordinate suppression of ERBB2 and ERBB3 in the human breast cancer cell line SKBR3 [13]. Furthermore, MiR-27b, which is expressed in MCF-7 cells, may be one of the causes of high expression of the drug-metabolising enzyme CYP1B1 in cancerous tissues [14]. Finally, as a tumor suppressor in breast cancer cells, miR-17-5p regulates breast cancer cell proliferation by inhibiting the translation of AIB1 mRNA [15].",1
18,18,"Research on the roles of BCSC-related miRNAs in breast cancer has great significance. Ponti [16] isolated tumorigenic breast cancer cells with stem/progenitor cell properties from a breast cancer cell line, and Huang [17] screened side population (SP) cells from a breast cancer cell line. Here, we investigated the miRNA expression profile of the ESA + CD44 + CD24 -/Low subpopulation from the MCF-7 cell line. Real-time RT-PCR was repeated three times, and the results were concordant with microarray data for the miRNA expression profiles of BCSCs.",1
18,19,"Recently, a few studies have reported miRNA expression in BCSCs. Shimono [18] found that 37 miRNAs were upregulated or downregulated in BCSCs compared to nontumorigenic breast cancer cells. Three clusters, miR-200c-141, miR-200b-200a-429, and miR-183-96182, were downregulated in human BCSCs. MiR-200c was shown to be overexpressed in MCF-7 cells, leading to reduced expression of transcription factor 8 and increased expression of E-cadherin [19]. Furthermore, the downregulation of Let-7 miRNAs rather than miR-200C was previously reported for human BCSCs [20]. Let-7 regulates multiple breast cancer stem cell properties by silencing more than one target, and Let-7 miRNAs are markedly reduced in BCSCs and increase with differentiation.",1
18,20,"We obtained miRNA expression profiles of BCSCs, providing a substantial basis for exploring the role of miRNAs in maintaining stem cell properties and the biological functions of BCSCs. Compared with previous reports, we found that miR-200C expression was about 3-fold lower in BCSCs than in MCF-7 cells as determined by Q-RT-PCR. Little change was observed in the expression of Let-7 family members, however, between BCSCs and MCF-7 cells, with the exception of Let-7e (data not shown). The discrepancies in Let-7 and miR200C expression between studies might be related to differences in tumor histology or the genetic backgrounds of the cell lines analysed. We also detected the expression of some predicted miRNAs in the BCSCs. Given that the existence of predicted miRNAs has yet to be validated, no accurate miRNA sequence could be used to synthesise accurate primers, making real-time RT-PCR verification unavailable. Further study of the functions of these characteristic BCSC miRNAs will facilitate research into the roles of miRNAs in breast cancer.",1
18,21,"Bioinformatic analysis and prediction programs have been the primary methods used to explore the function of miRNAs [21,22]. The genes possibly regulated by these characteristic BCSC miRNAs are involved in both tumorigenesis and stem cell maintenance. For example, miR-122a has been reported to be specific to liver tissue [23,24]; however, our results showed upregulation of miR-122a in BCSCs. The microarray data were verified by Q-RT-PCR. Furthermore, miR-122a was also detected in MCF-7 cells in the Ambion dataset. Bioinformatic analysis showed that the potential targets of miR-122a include several cancer-related genes. In previous reports, it has been shown that miR-122a plays a role in the genesis of hepatocellular carcinoma by blocking cyclin G1 expression [25]. Another study found that G3BP2, one of the potential targets of miR-122a, was more highly expressed in breast cancer tissue than in paraneoplastic tissue [26-28]. These studies indicate that miR-122a is likely to be an important gene regulatory factor in cancer cells, even cancer stem cells. Another example is miR21, which has been reported to have extensive roles and is expressed in embryonic stem cells [29], neuronal cells [30] and several tumor tissues [31,32].",1
18,22,"Interestingly, target analysis of miR-21 revealed two classes of genes with opposite functions, e.g., PLAG1 (pleiomorphic adenoma gene 1) and PDCD4 (Programmed cell death 4). As a cancer-promoting gene, PLAG1 plays an essential role in the processes of adenocarcinoma formation and malignant transformation in various types of tumors [35], whereas PDCD4 is a tumor suppressor gene that inhibits neoplastic transformation and tumor cell invasion and facilitates apoptosis [36]. Several recent studies have shown that the tumor suppressor PDCD4 is a target of miR-21 [37-39]. Nevertheless, the question remains whether PLAG1 is likely to be a target of miR-21. Moreover, the potential target genes of miR-21 include several oncogenes such as RAB11A, RAB6A, RAB6C, RASGRP1, RHOB and RASA1, etc. Are these genes the true targets of miR-21? What are the mechanisms of their involvement in the genesis of breast cancer? These intriguing questions remain to be answered.",1
18,23,"Furthermore, the prediction of potential targets for other BCSC-related miRNAs indicated overlap between the targets of different miRNAs. For example, PLAG1 was a potential target for both miR-224 and miR-200a, and the expression of miR-200a was lower in BCSCs than in MCF-7 cells. In contrast, the expression of miR224 was higher in BCSCs than in MCF-7 cells. It is likely that the miRNAs that are over-expressed or under-expressed in BCSCs may regulate common target genes and form a miRNA gene network by cooperating or competing with each other to regulate the development of BCSCs.",1
18,24,"Moreover, miR-301, miR-296, miR-21 and miR-373* have been reported to be expressed in human embryonic stem cells and other stem cells, indicating that these miRNAs may play a constitutive role in maintaining the biological characteristics of stem cells [40,41]. Future work should include verification of the potential targets of all of the BCSC-related miRNAs identified here.",1
18,25,"Here, we investigated the miRNA expression profile of the ESA + CD44 + CD24 -/Low BCSC subpopulation from the MCF-7 cell line. Our identification of BCSCrelated miRNAs should be a starting point to explore the functions of these miRNAs, adding a new dimension to our understanding of the complex picture of BCSCs and assisting cancer biologists and clinical oncologists in designing and testing novel therapeutic strategies.",1
14,1,"The evaluation of the likelihood of breast cancer recurrence depends on various factors that help determine a patient's risk. These factors, combined with predictive elements like estrogen-receptor status, help make the best decisions related to adjuvant systemic therapy. By balancing the potential benefits and consequences of treatment, optimal treatment decisions can be made. That's why it's crucial to have precise prognosticators that can accurately define a patient's risk category for breast cancer.",0
14,2,"Axillary lymph node status is currently the most significant factor for predicting breast cancer prognosis in women. For node-positive patients, there is a higher risk of distant recurrence based on the number of involved nodes. However, recommendations for systemic adjuvant chemotherapy are not always clear-cut even with the usefulness of lymph node status. For instance, five-year survival rates indicate that around 15% of node-negative patients with larger tumor sizes (>1 cm) may benefit from systemic adjuvant therapy, while about 85% can survive without it. Additionally, local-regional therapy may result in one-third of node-positive patients being free of recurrence.",0
14,3,"Prognostic risk factors that play a major role in node-negative patients include tumor size and histological tumor grade [1-4,9,10]. Tumor size significantly affects the outcome of node-negative patients and is usually considered while deciding on adjuvant treatment [6,11]. Tumor grade, on the other hand, is used to make decisions in cases where tumor sizes are borderline [1,2,5]. However, grading remains a challenging task due to significant inter-observer variation [12-14] as pathologists need to assess complex histological characteristics in a semi-quantitative manner.",0
14,4,"The loss of structural organization and functional coordination in invasive breast cancer can be seen through the increase in morphologic complexity of the tissue components at different levels. This complexity can be measured and related to patient outcome using fractal analysis, which is a valuable tool in quantifying complex pathological structures. In this study, the prognostic value of a new technique that measures the fractal dimension of histological structures of breast tissue microarray cores is being assessed. The technique uses pan-cytokeratin staining to highlight the epithelial architecture.",0
14,5,"A retrospective selection of 408 patients diagnosed with primary invasive ductal carcinoma (IDC) of the breast from the Calgary Regional Hospitals was conducted with institutional ethics approval from the Institutional Review Board (IRB). Patient consent was not required as it was a retrospective study with a low risk of exposing patient confidentiality, especially for deceased patients. Out of these patients, 379 had at least one of three TMA cores which was adequately stained for fractal analysis. The mean and median age at diagnosis for these patients were 65 and 66, respectively, with an age range of 34 to 95. Stage information was available for 375 of these patients, with the majority of patients being Stage I (60.0%), followed by Stage II (26.4%), and Stage III (13.6%). All patients selected had received adjuvant tamoxifen treatment between 1988 to 2006, and were identified through Alberta Cancer Board records of patients who had received tamoxifen treatment without chemotherapy. The inclusion criterion was any patient who had adequate tissue for TMA construction and received adjuvant tamoxifen treatment but no adjuvant chemotherapy.",0
14,6,"Hemotoxylin and Eosin (H&E) stained sections were utilized to identify areas of tumors for TMA cores. A total of fourteen breast TMA blocks, each containing an average of 94 tissue cores, were created from untreated breast cancer tissue that had been fixed in formalin and embedded in paraffin. To ensure no bias was present, three 0.6 mm cores were randomly selected from cancerous areas of each donor block to form the recipient TMA core block. The Leica RM2235 microtome was used to cut 4 μm thick sections from each TMA donor block. In a previous study on prostate cancer specimens, we determined that pan-cytokeratin stained specimens provided more reliable classification performance compared to serial sections of the same specimens stained with H&E. This is because pancytokeratin isolates and highlights the morphology of epithelial components while excluding structures that do not express pathological relevance in the form of morphologic complexity (i.e. connective tissue components). Thus, all TMA sections were stained using Ventana Benchmark LT. Protease 1 antigen retrieval was utilized and followed by Ventana pre-diluted pan-cytokeratin antibody (cat. No. 760-2135) with an incubation time of 32 minutes.",0
14,7,"The TMA cores were examined using an optical microscope (Zeiss Axioscope) with a 10× objective and imaged with an AxioCam HR digital camera (Carl Zeiss, Inc.) at the camera's native resolution of 1300 × 1030 pixels. The AxioCam HR has pixels of size 6.7 μm × 6.7 μm, which are 1.06 μm × 1.06 μm in apparent size at the combined magnifications of 10× objective and 0.63× C-mount optical coupling. The acquired images were saved in tagged image file format (tif).",0
14,8,"Fractal dimension differs from traditional dimension in that it can be a non-integer value. The more complex the form of an object, the higher its fractal dimension compared to its topological dimension (refer to Figure 1). Fractal dimension measures the degree of structural complexity by evaluating changes in detail level as the structure is viewed at different scales [19]. This makes it well-suited for describing asymmetrical structures that maintain a constant level of complexity across various scales.",0
14,9,"The morphologic complexity of breast epithelium, a pathologically relevant histological feature, was quantified using an automated fractal analysis technique that was previously developed [18]. To summarize, the technique involves the following steps:",0
14,10,"The tissue specimens are treated with a histological stain to emphasize and isolate the histological structures of concern, such as the outlines of multi-cellular structures (gland formations), individual cell shapes, and sub-cellular structures (distribution of keratin within the cells and nuclear shape).",0
14,11,The stained specimens were acquired and their background was corrected by capturing a "blank" image under the same imaging conditions as the TMA images. The non-uniform background luminance was subtracted using the "blank" image [18]. The resulting background corrected images were then converted to grey-scale (Figure 2).,0
14,12,"The grey-scale version of the image specimen is converted into a series of binary images by applying a series of intensity thresholds. Histological morphology outlines are then derived from these images, as shown in Figure 2. Figure 3 provides a closer look at a magnified region of Figure 2A, highlighting the segmented morphology outlines in greater detail.",0
14,13,"Apply the box counting method (with a spatial scale range of 10 to 50 μm) to determine the fractal dimension of each outline image obtained in step 3. [19, 20]",0
14,14,The fractal dimension of the pathological morphology can be identified by determining the global maximum point on a plot of fractal dimension against intensity threshold.,0
14,15,"In our prior study, we demonstrated that our approach for determining the fractal dimension remains constant despite alterations to microscope lighting or variations in stain uniformity and intensity [18]. Furthermore, it is important to highlight that the fractal dimension remains unchanged when the magnification is adjusted, as long as the specimen image's field of view maintains the scale range of the structures of interest over which the fractal dimension was established.",0
14,16,"Our fractal analysis method was performed on a total of 1224 TMA cores, with 3 cores taken from each of the 408 patient samples. The TMA core with the greatest fractal dimension was chosen for statistical analysis to avoid the possibility that other cores from the same patient contained only benign or highly differentiated tissue. By selecting the core with the highest fractal dimension, we anticipated it would be representative of the most malignant neoplasm with the greatest deviation from normal cellular/glandular breast morphology, indicating abnormal and/or aggressive tumor growth with potential for metastasis.",0
14,17,"Of the 408 patients, fractal dimension was successfully measured in at least one of the three TMA cores for 92.9% (379) of them. The remaining 29 patients' specimens could not be assessed due to insufficient staining or folding. Eight of those 29 patients had all three TMA cores result in a blank slide. Overall, 9.5% (36), 27.7% (105), and 62.8% (238) of patients had one, two, and three evaluable cores, respectively.",0
14,18,"It can be helpful to convert a measured variable into a categorical variable to assign patients to different risk categories for analysis purposes. Since the fractal analysis technique used in this study is new and there are no standard cutpoints to follow, different methods can be used to determine them, such as biological, data-oriented, or outcome-oriented approaches. However, there is no consensus on which method is the best. In this study, the data-oriented method was applied, and two cutpoints were chosen based on the fractal dimension data. One cutpoint was set at the upper quartile (75th percentile) of the data, and the other was set at the median of the remaining lower three-quarters of the data. Using two cutpoints, rather than one, helps to evaluate whether there is a graded association between fractal dimension and patient prognosis.",0
14,19,"Associations between fractal dimension scores grouped into categories and clinicopathological variables were evaluated for statistical significance with a chi-square test. To determine statistical significance, Kaplan-Meier methods were utilized to calculate the 10-year disease-specific and overall survival rates, and the logrank test was used to compare the curves. Disease-specific survival was measured from the time of cancer diagnosis until the date of death from cancer or last follow-up. Overall survival was measured from the date of cancer diagnosis until the date of death from any cause or the last follow-up. The above statistical analyses were replicated using Cox proportional hazards regression modeling to determine whether any of the clinicopathological variables affected the results. The proportional assumption was tested for all covariates using Log-Minus-Log Survival Plots, and no violations of the assumption were observed. SAS 9.2 software (SAS Institute Inc) was utilized to perform the statistical analyses.",0
14,20,"The area under the curve (AUC) from a receiver operating characteristic (ROC) analysis was used to quantify the prognostic accuracy of fractal dimension in predicting death from breast cancer and death from any cause. The AUC values ranged from 0.5 (chance accuracy) to 1.0 (perfect accuracy), with intermediate benchmarks of 0.6 (fair), 0.7 (good), 0.8 (excellent), and 0.9 (almost perfect). The predicted probability of outcome from a Cox regression model was considered as a continuum for the analysis, while the actual occurrence of outcome was used as the comparative standard.",0
14,21,"A method of split-sample cross-validation was applied to determine how well the findings could be applied to other populations. The initial population consisted of 379 patients and was randomly divided into two groups, one for training and the other for validation, with 190 and 189 patients, respectively. A regression equation was developed using the training set and the AUC was estimated by comparing the observed and predicted results. The predicted values for the validation set were then determined using the regression coefficients obtained from the training set, and the AUC was again calculated. The difference between the AUCs for the training and validation sets was used to obtain the shrinkage coefficient, which provides a measure of confidence in the generalization of the results. While there are no clear-cut standards for what constitutes an ideal shrinkage coefficient, lower values are preferred. The data from both sets were combined and a final regression equation was obtained based on the entire sample once an acceptable shrinkage coefficient was achieved.",0
14,22,"Instead of excluding patients with missing data, which numbered 15 for tumor grades, 4 for lymph node status, 15 for estrogen-receptor status, and 12 for HER-2/neu status out of 379 evaluable patients, the predicted mean approach in SOLAS 3.0 software was used to impute the data. This allowed for a larger sample size and any imputation bias was assessed by re-running the analysis without including any patients with missing data. The results were similar, so the analysis was reported with the imputed data.",0
14,23,"The fractal dimension scores ranged from 1.08 to 1.97, with a median of 1.62, lower quartile of 1.49, and upper quartile of 1.75. The cores were moderately related with an intraclass correlation of 0.51. Two cutpoints were selected using a data-oriented approach; fractal dimension values below 1.56 were considered low (N = 141), 1.56-1.75 as intermediate (N = 148), and above 1.75 as high (N = 90). Representative TMA cores from these fractal dimension categories are shown in Figure 2, which depicts the increasing complexity of outline morphology in the low, intermediate, and high fractal dimension categories (A-C).",0
14,24,"Table 1 displays the baseline patient features. Greater fractal dimension correlated significantly with traditional poorer prognosis indicators, such as advanced age, larger tumor size, higher tumor grade, and positive lymph node status. Nonetheless, fractal dimension did not associate with either estrogen receptor status or HER-2/neu status.",0
14,25,"The patients were followed up for a median of 5.2 years. The disease-specific and overall survival rates for the entire group of 379 patients were 52.5% and 42.5%, respectively, after 10 years. Patients with higher fractal scores had worse disease-specific survival than those with lower scores (25.0% versus 56.4% versus 69.4%, p < 0.001; Table 2 and Figure 4A). Additionally, patients with higher scores had worse overall survival (14.2% versus 39.9% versus 67.4%, p < 0.001; Table 2 and Figure 4B). The prognostic accuracy of fractal dimension was good, with AUCs of 0.66 and 0.67 for univariate disease-specific and overall survival, respectively. Older age, higher grade, and positive lymph node status predicted worse outcomes, but not the size of the tumour, estrogen-receptor status, or HER-2/neu status (Table 2).",0
14,26,"The tumour grade was determined using reports from 10 to 30 certified cancer pathologists. Despite a clear separation in disease-specific survival for different fractal dimensions (shown in Figure 4A), there was significant overlap in survival curves for grade 1 and 2 tumours over the entire 10-year follow-up period (see Figure 4C). Additionally, the overall survival curves for grades 1 and 2 were nearly identical for the first four years following diagnosis (Figure 4D). These findings indicate that tumour grade 1 and 2 cannot accurately predict outcomes for patients over the course of 10 years.",0
14,27,"Results from the Cox proportional hazards regression analysis indicate that fractal dimension remains a noteworthy prognostic indicator even after adjustment for all clinicopathological variables (as seen in Table 3). Despite the smaller multivariate hazard ratio (Table 3) relative to the univariate hazard ratio (Table 2), this finding implies that fractal dimension is a robust prognostic factor. The AUCs for the 7-factor regression models highlight that fractal dimension continues to be a strong predictor of disease-specific and overall survival, with little contribution to prognostic accuracy from the other clinical-pathological factors. It is worth noting that even when comparing the combined grades of 1 and 2 to grade 3 tumors, fractal dimension remained significantly more strongly associated with both disease-specific and overall survival.",0
14,28,"'The study used split-sample cross-validation to evaluate the generalizability of the results outlined in the statistical analysis section. Table 4 shows that the results from both sets align with each other as well as the complete sample, including similar frequency distributions and 10-year survival rates across low, moderate, and high fractal dimension categories. The study discovered that higher levels of fractal dimension doubled the hazards, which remained consistent even with smaller sample sizes. Both disease-specific and overall survival had negative shrinkage coefficients of -0.01 and -0.05, respectively. As a result, the researchers concluded that fractal dimension is generalizable and that it was acceptable to combine data from both sets in the analysis.'",0
14,29,"We developed a method to measure the morphologic complexity of epithelial architecture and found a direct association between fractal dimension and breast tumour grade. In this study, we analyzed specimens from patients with invasive breast cancer and found that fractal dimension showed a stronger association with disease-specific survival than standard clinical prognosticators. This is the largest and only study of its kind investigating a positive association between breast epithelial architecture and patient outcome. Fractal dimension has potential advantages over conventional tumour grading as a quantitative and reproducible indicator, providing pathologists with rapid and cost-effective analysis from as few as three TMA cores per patient.",0
14,30,"Studies examining the prognostic value of potential factors should ideally only include patients who have not yet undergone systemic therapy. However, this is becoming increasingly challenging because adjuvant therapy is now recommended for a wider range of breast cancer patients. This particular study did not involve the use of adjuvant chemotherapy, but all patients received adjuvant tamoxifen therapy, regardless of their ER status. Even though this may affect the results, it is expected that the fractal dimension will be independent of the tamoxifen therapy since all patients received the same treatment. The results seem to support this since the percentage of patients with low, intermediate, and high fractal dimension is similar across ER-positive groups, suggesting that tamoxifen therapy has put all patients with ER-positive breast cancer on an equal footing.",0
14,31,"Fractal analysis has previously been used to characterize cancer, and some studies have shown that fractal dimension can describe the complex structures seen in some cancers. However, this study is the largest and only one to relate fractal dimension of epithelial architecture to patient outcome. Although an external patient validation set was not used in this proof of principle study, a data-oriented approach was used to minimize bias in cutpoint selection, and a cross-validation analysis was conducted, suggesting the results are generalizable. Higher fractal dimensions were associated with a poorer outcome, indicating the potential of fractal dimension as an image-based prognostic marker. This is consistent with the idea that malignant breast neoplasms demonstrate a lack of structural organization and functional coordination with surrounding normal tissue. Additionally, changes in the morphological complexity of architectural components of a neoplasm that arise from changes in the functional status of cells can be quantified using fractal analysis.",0
14,32,"The results of this retrospective study suggest that fractal dimension is a promising image analysis marker for predicting the prognosis of IDC of the breast. However, external validation studies and controlled prospective clinical trials are needed to confirm its prognostic value. To further investigate its potential, future work will examine the use of fractal dimension to define risk categories for Stage I, lymph node-negative and tumour size ≤ 2 cm in maximum diameter, ER-positive breast cancer patients who have not received adjuvant systemic therapy. Additionally, these studies will explore the combination of fractal dimension with a quantitative analysis of mitotic count to validate the significance of morphologic complexity of epithelial architecture in node-negative breast cancer, and examine the synergy between morphologic complexity and cellular proliferation.",0
16,1,"Tooth loss, which is an indicator of periodontal disease, has been linked to Alzheimer's disease and dementia. Those with clinical dementia often experience a decline in dental health, leading to potential nutritional deficiencies. Animal models have shown that molar loss can result in reduced pyramidal cells and acetylcholine levels in the hippocampus. Additionally, it has been proposed that inflammatory molecules and bacteria from periodontal disease can cause brain inflammation.",0
16,2,"Individuals with mild memory impairment (MMI) have a high risk of developing dementia, with 21.2% progressing to illnesses such as Alzheimer's disease, vascular dementia, or other types of dementia over a period of 5 years. MMI is defined as having normal general cognitive function and no impairment of daily activities, but with objective memory impairment as assessed by the MMSE recall test, and absence of dementia or depression diagnosed by geriatric neuropsychiatrists according to DSM-III R criteria.",0
16,3,"To investigate the potential link between tooth loss and early stages of Alzheimer's disease (AD) and dementia, a community-based survey was conducted. Individuals with Mild Cognitive Impairment (MMI) or a low score on the Mini-Mental State Examination (MMSE) were identified using the MMSE and the Geriatric Depression Scale (GDS) short version. Various data points were collected, including the number of remaining teeth, length of time since tooth loss, health-related lifestyles, and medical history. Additionally, physical measurements such as blood pressure, height, and body weight were taken, along with fasting blood samples. Through this cross-sectional study, the goal was to compare the number of remaining teeth and duration of tooth loss in subjects with MMI or a low MMSE score against those in a control group of elderly individuals.",0
16,4,"The Medical Ethics Committee of Nara Medical University approved this study, and each subject provided written informed consent before participating.",0
16,5,"Based on the baseline examination data of the Fujiwara-kyo study, our focus was on elderly men and women aged 65 years or more, residing in their respective homes in Nara prefecture where the first capital of Japan known as ""Fujiwara-kyo"" was founded. The subjects voluntarily participated and could walk independently. 4,206 individuals gave written consent and completed the baseline examination within the timeframe of 2007-2008.",0
16,6,"The MMSE is employed to identify cognitive impairment, with scores ranging from 0 to 30. The Recall test, which assesses recent memory impairment, is a component of the MMSE and has a score range of 0 to 3. Participants were instructed to recollect three unrelated objects that they had been asked to remember beforehand. The MMSE was administered by clinical psychologists or interviewers who had received formal training from a psychiatrist and completed graduate studies in psychology.",0
16,7,"Depression was assessed in the study using the GDS, which has a score range of 0-15 and a cut-off score of 5/6. Participants were asked to indicate whether they experienced specific symptoms (coded as 1 for present and 0 for not present), and their responses were added to obtain a final score. Higher scores on the GDS indicate a greater number of depressive symptoms. The GDS was administered as part of a self-administered questionnaire survey.",0
16,8,"The dental exam involved two dentists trained in the same techniques, with both the dentist and patient seated and under artificial lighting. The number of teeth and their condition were recorded, excluding teeth marked for extraction or root tips. The age of edentulous patients was also recorded along with the Community Periodontal Index code. The upper and lower teeth were divided into segments, and ten representative teeth in six segments were examined and assigned a code between 0 and 4 based on their condition. Ineligible segments had one or no remaining teeth.",0
16,9,"The participants were queried regarding their alcohol consumption habits, including the frequency of drinking, the type of alcohol consumed, and the average daily intake. They were also asked about their smoking history and daily walking time in a self-administered questionnaire which was further confirmed by an interview. Moreover, the researchers interviewed each participant to determine any impairment of their daily activities, such as eating, dressing and walking. The presence of any medical history, including diseases such as cancer, cerebrovascular disease, myocardial infarction, diabetes mellitus, and hypertension was also recorded. The participants' blood pressure was then measured twice at 30-second intervals using an automatic blood-pressure manometer after resting quietly for over five minutes. The device displayed results from the Korotkoff method.",0
16,10,"Statistical analyses were conducted to compare the control group with the MMI group and the low MMSE score group. The significance level was set based on the Bonferroni test. The Mantel-extension method was utilized to identify any correlation between the number of remaining teeth, CPI, and the prevalence of MMI or low MMSE scores. Logistic regression analysis was performed using depressive symptoms, sex, age, length of education, and other independent variables. The number of remaining teeth and the length of time after becoming edentulous were used as continuous and categorical variables in separate models. The Hosmer and Lemeshow technique was employed to evaluate goodness of fit. SPSS (version 17.0) was used for all statistical analyses. The P values were calculated for all the analyses, and the level of significance was set at 0.05.",0
16,11,"The process for subject selection is illustrated in Figure 1. Out of the 4,206 participants who underwent examination, we excluded 145 individuals with severe visual or hearing impairments that could affect cognitive function tests. After removing these participants, 214 out of the remaining 4,061 individuals had low MMSE scores (i.e., 23 or lower). To ensure that our remaining sample reflected individuals with independence in basic and instrumental ADL skills, we interviewed the remaining 3,847 participants who scored 24 or higher on the MMSE. Their Recall scores were used to categorize 151 participants as low-scoring (score of 1 or 0) and 3,696 as high-scoring (score of 3 or 2) control group participants. We additionally excluded 30 participants from the low-scoring group who displayed signs of depression based on GDS scores of 6 or more, as this could lead to pseudo-dementia. Thus, the final analysis consisted of 3,696 control group participants, 121 participants with MMI scores, and 214 participants with low MMSE scores. This was a cross-sectional study.",0
16,12,"Table 1 displays the demographic traits of the participants. Significant differences in factors, as determined by the Bonferroni test, were observed between the control group and the MMI group or those with low MMSE scores. These factors include depressive symptoms, age, gender, length of education, remaining teeth count, alcohol consumption of at least once a week, smoking, daily walking time, serum albumin levels, total cholesterol levels, and low-density lipoprotein cholesterol levels.",0
16,13,"The prevalence of MMI or a low MMSE score is illustrated in Figure 2. The remaining teeth were divided into three categories - 22-32, 11-21, and 0-10. No significant dose-response relationship was observed in a trend test between the number of remaining teeth and MMI prevalence. However, individuals with 0-10 remaining teeth had a higher MMI prevalence than those with 22-32 remaining teeth. The prevalence of a low MMSE score increased in both age categories as the number of remaining teeth decreased. A significant increase in prevalence was observed in a trend test (P < 0.001). The CPI code did not show a significant dose-response relationship with MMI or low MMSE prevalence. This is shown in Figure 2's lower panel.",0
16,14,"We used multivariate adjusted odds ratios (Table 2) to determine if the presence of fewer remaining teeth was independently associated with MMI or a low MMSE score status. We only used factors with a calculated P value < 0.05 between the control and MMI groups or between the control and low MMSE score groups in Table 1. After adjusting for variables in the full models, we found a significant relationship between the number of remaining teeth (per 1 decrease) and MMI (odds ratio: 1.021, 95% CI: 1.001-1.041), and a low MMSE score (odds ratio: 1.039, 95% CI: 1.0231.054). Moreover, the odds ratio of having 0-10 remaining teeth to 22-32 remaining teeth was 1.679 (95% CI: 1.073-2.627) for MMI and 2.177 (95% CI: 1.510-3.140) for a low MMSE score.",0
16,15,"Among individuals who were edentulous, the median age at which they became edentulous and the prevalence of those who were edentulous for 15 years or more were compared between control, MMI, and low MMSE score groups. The results showed significant differences between the control and low MMSE score groups. After adjusting for multiple variables, the odds ratios for a low MMSE score were found to be higher for both ""per 1 year increase"" and ""15 years or more.""",0
16,16,"The study conducted within the community inferred that a decrease in the number of remaining teeth was linked with a significant rise in the occurrence of a low MMSE score. The results were confirmed after considering other explanatory factors, affirming the association between the decrease in the number of remaining teeth and low MMSE score. These findings were in line with earlier reports that showed the connection between tooth loss and reduced cognitive function. Furthermore, the study showed that a decrease in the number of remaining teeth was linked with the risk of MMI.",0
16,17,"The present study has several limitations that require consideration. Firstly, the data were obtained from a cross-sectional study, preventing us from making definitive conclusions about the biological credibility of the effect of tooth loss on MMI and a low MMSE score. Secondly, we did not explore the timing or causes of tooth loss. The severity of periodontal disease could have been more accurately assessed if we had examined markers of inflammation in the gingival crevicular fluid or alveolar bone loss measurements, rather than using the CPI code. Thirdly, our identification criteria for MMI differed from those established by Ishikawa, as we employed MMSE and GDS screening tests to identify the absence of dementia or depression. Nonetheless, the prevalence of MMI in our study was similar to that of Japanese community-based MCI. Finally, we did not genotype for APOE, which has been linked to dementia in Japanese-American men.",0
16,18,"The relationship between tooth loss and a low MMSE score can be explained by the fact that older adults with a low MMSE score tend to not frequently use dental services. Furthermore, individuals with dementia tend to have increased plaque accumulation. It is possible that other biological factors besides dental health deterioration caused by cognitive impairment contribute to the relationship between tooth loss, MMI, and a low MMSE score, as evidenced by the significant association between tooth loss and MMI subjects who retain their basic and instrumental ADL.",0
16,19,"Four additional potential biological explanations can be suggested for the correlation between tooth loss and cognitive function. To begin with, systemic inflammation resulting from periodontal disease, a leading cause of tooth extraction in the elderly, may link to cognitive function through cytokines that cross the blood-brain barrier. Furthermore, periodontal disease and cognitive function have been linked to genetic risk factors, such as an interleukin 1 gene polymorphism. Tooth loss may also result in a decrease in the number of periodontal mechanoreceptors, which are sensory receptors, causing a memory and learning disorder. Despite animal models indicating functional deterioration of the cholinergic neuronal system and a negative impact on the number of tyrosine kinase B mRNA-positive cells with the duration of tooth loss and number of extracted teeth, it is important to conduct further research.",0
16,20,"A meaningful correlation was established between the duration of being without teeth and the likelihood of receiving a low MMSE score, even after accounting for their age during the initial examination and other factors (refer to Table 3). AD was more prone to occur in those who had ""lost all teeth or lost half of teeth before age 35"" [3]. While no significant link was observed, the MMI group had a more elevated occurrence of those who had been edentulous for 15 years or more (50.0%) versus the control group (34.4%). As a result, losing teeth may cumulatively harm the brain.",0
16,21,"There was a significant correlation between the number of remaining teeth and the risk of MMI and a low MMSE score, but no significant correlation between the CPI code, MMI, and a low MMSE score was found. Table 1 and Figure 2 illustrate this. The CPI underestimates the relationship between cognitive function and the cumulative burden of periodontal disease. Tooth loss or periodontal disease could be related to cognitive function because the prevalence of ""Ineligible for CPI"" was higher and ""Code 0, 1, or 2"" was lower in the MMI and low MMSE score groups than in the control group.",0
16,22,"The length of the edentulous period and the number of remaining teeth were found to have a significant correlation with cognitive function. Although APOE genotyping data was not available, this cross-sectional study implies a noteworthy association between tooth loss and MMI as well as cognitive impairment. Additional research is necessary to explore the precise factors that contribute to this connection.",0
18,1,"Breast cancer is a common cancer in women that poses a threat to their health. According to Al-Hajj's research in 2003, breast cancer stem cells with self-renewal and multi-directional differentiation properties (ESA+CD44+CD24-/low, BCSCs) are the primary contributors to drug resistance, recurrence, and metastasis of breast cancer. Previous studies have also identified that breast cancer stem cells possess an ESA+CD44+CD24-/low phenotype. This study is based on these findings and further confirms them. Research on BCSCs is expected to bring revolutionary changes in our understanding of breast cancer; however, several issues related to the molecular basis of carcinogenesis remain unresolved. We need to investigate the full nature of BCSCs' involvement in the molecular mechanisms of tumorigenesis, including the role of microRNAs (miRNAs).",0
18,2,"miRNAs, a crucial type of regulatory noncoding RNAs, have an essential function in the committed differentiation and self-renewal of embryonic stem cells and adult stem cells. The most recent miRBase release (10.0) contains 5071 miRNA loci from 58 unique species. miRNAs can either act as oncogenes or anti-oncogenes and are associated with the growth of various types of tumors, such as chronic lymphocytic leukemia, paediatric Burkitt’s lymphoma, gastric cancer, lung cancer and large-cell lymphoma. In humans, miRNAs (1048 sequences in miRBase 16, Sep 10th, 2010) regulate more than one-third of all genes, bringing hope to studies of cancer stem cells. Therefore, the identification of cancer stem cell-related miRNAs would provide valuable information for a better understanding of cancer stem cell properties and the molecular mechanisms of carcinogenesis. To explore this, the miRNA expression profiles of ESA + CD44 + CD24 -/low BCSCs were probed from the MCF-7 cell line.",0
18,3,"The minimal essential medium (MEM) from Invitrogen (America) was used to culture the MCF-7 human breast cancer cell line. Cells in log phase were dissociated with 0.25% trypsin from Gibco (America), washed with PBS, and stained with FITC-conjugated anti-ESA, APC-conjugated anti-CD44, and PE-conjugated anti-CD24 from BD PharMingen (America). After a 30-minute incubation, the cells were washed thrice, and FACS (MoFlo, America) was performed to isolate the ESA+CD44+CD24-/low cells.",0
18,4,"The ESA+CD44+CD24-/low lineage- cells that had been isolated were suspended in MEM supplemented with 1% FBS and were rinsed twice using the same medium. After that, EpiCult™-B medium (Stemcell Technologies, Canada) fortified with 5% FBS was used to replace the medium. Next, NIH/3T3 feeder cells that had been exposed to 60 Co at 50 Gy were seeded onto 24-well plates at a density of 2 × 10^4 cells or 1 × 10^4 BCSCs. The mouse embryonic fibroblast cell line NIH/3T3 was cultured in DMEM (Invitrogen). After 24 hours of seeding, the medium was changed again to serum-free EpiCult™-B medium for the cells and they were incubated in 5% CO2 at 37°C. Fresh medium was given to the cells every 3 days, and colonies appeared under a microscope after 7-10 days.",0
18,5,"Female NOD/SCID mice aged eight weeks were subjected to 2.5 Gy of 60Co radiation, after which tumor cell injections were conducted one day later. The tumor cells were suspended in 0.2 ml of IMDM, which contained 10% FBS, and were then injected into the left armpit's mammary fat pad. The test group received injections of 0.5 x 10^3, 1 x 10^3, 5 x 10^3, 1 x 10^4, or 5 x 10^4 ESA+CD44+CD24-/low cells that were segregated via FACS. Meanwhile, the control group mice were given injections of 1 x 10^4, 5 x 10^4, 1 x 10^5, 5 x 10^5, or 1 x 10^6 MCF-7 cells. Each group was given three mice, which were inoculated with the same number of cells. Over eight weeks, the mice were tracked for tumor growth every ten days, after which they were euthanized through cervical dislocation. A single-cell suspension was obtained as per a previously published protocol, after which ESA+CD44+CD24-/low cells were removed from the xenograft tumor cells via FACS and injected into the mammary fat pad, as had previously been described. The Third Military Medical University's Animal Ethics Committee approved all animal procedures.",0
18,6,"Tissue slides of the tumor were made ready for immunohistochemistry using already published protocol [9] and the markers of the luminal epithelial and myoepithelial cells, which are Epithelial membrane antigen (EMA) and smooth muscle actin (SMA) respectively, for the immunostaining. Anti-EMA or anti-SMA antibodies, that were rabbit polyclonal and diluted to 1:500, were used, and Santa Cruz of CA provided them.",0
18,7,"The process of making and testing miRNA microarrays was conducted according to methods previously reported [9]. The microarray utilized in this study consisted of 517 mature miRNA series and 122 predicted miRNA (Pred_miR) series from published sources [10]. Each sample was tested twice, with three identical miRNA probes on the microarray for each test. The SAM method (version 2.1) was employed for the microarray data analysis, using a two class unpaired comparison approach.",0
18,8,"The primers used in this study were designed with Primer Express version 2.0, following the protocol by Chen et al. [11] for primer design and realtime RT-PCR. For the internal control, the primers 5’-ctcgcttcggcagcaca-3’ and 5’-aacgcttcacgaatttgcgt-3’ were designed for the U6 small nuclear RNA. The miRNAs analysed in this study included miR-122a, miR-188, miR-200a, miR-21, miR-224, miR-296, miR-301, miR-31, miR-373*, and miR-200C.",0
18,9,"Three bioinformatic programs were utilized for target prediction of the miRNAs, namely miRanda http://microrna.sanger.ac.uk, picTar http://www.ncrna.org/KnowledgeBase/link-database/mirna_target_database, and targetscan http://www.targetscan.org.",0
18,10,"The flow cytometry analysis was used to evaluate the expression of ESA, CD44 and CD24 in MCF-7 cells. A small percentage of lineage-negative ESA+ CD44+ CD24-/low cells were identified and isolated through FACS sorting with high purity. The clonogenic potential of these BCSCs was evaluated through culturing them on irradiated NIH/3T3 feeder cells. The cells formed epithelioid colonies within 3-6 days and continued to expand over time. The ESA+ CD44+ CD24-/low cells exhibited no distinct morphological characteristics compared to MCF-7 cells. A detailed account of these findings is provided in Figure 1.",0
18,11,"We subcutaneously injected ESA + CD44 + CD24 -/low cells and MCF-7 cells (as a control) into the armpits of NOD/SCID mice. Following 8 weeks, MCF-7 cells formed new tumors only when ≥5 × 10^5 cells were administered, failing to do so at lower doses (1 × 10^5 cells). In contrast, ESA + CD44 + CD24 -/low cells formed tumors in three of three, three of three, and one of three animals when 5 × 10^4, 1 × 10^4, and 5 × 10^3 cells were injected, respectively. Tumor specimens were collected and passaged into recipient mice, with tumors forming in three of three, three of three, and two of three recipient animals when 5 × 10^4, 1 × 10^4, and 5 × 10^3 cells were administered, respectively. One of three animals in the control group displayed tumors when administered 5 × 10^5 cells, but 5 × 10^4 -1 × 10^5 cells did not generate tumors in the control group (Table 1 Figure 1C). These findings suggest that ESA + CD44 + CD24 -/low cells have a significant tumorigenic potential and are capable of initiating tumors up to 100- to 1000-fold more efficiently than MCF-7 cells.",0
18,12,"The variability of ESA+CD44+/CD24- subpopulation in the murine model was tested through FACS analysis. The unsorted MCF-7 xenografts showed a constant occurrence of 1-2%, while the sorted MCF-7 xenografts exhibited a significant increase in the subpopulation, reaching 4-5%.",0
18,13,"Tissue slides from the tumors were made and subjected to H&E and immunohistochemical staining. The BCSCs tumors showed positive results for both EMA and SMA, indicating the presence of luminal epithelial and myoepithelial cells. Meanwhile, the MCF-7 control group tumors only showed positive results for EMA, suggesting the presence of luminal epithelial cells but not myoepithelial cells (see Figure 2).",0
18,14,"The hybridisation reaction was conducted twice for each type of cell. The U6 snRNA internal controls on all microarrays produced consistent signal strength, and the signal intensity of all detected spots on replicate microarrays showed high R values (R = 0.9747 ± 0.0304), indicating reliable hybridisation between the replicas (see Additional file 1 Figure S1). In total, 147 miRNAs were identified in MCF-7 cells and 102 miRNAs in BCSCs, including predicted miRNAs (PRED_MIR) with a signal value above 800. Out of the 41 miRNAs reported by Ambion in its MCF-7 cell miRNA expression profile (signal value ≥++), 34 were also detected in this study, demonstrating a concordance rate of 82.9% (see Additional file 1 Table S1 S2 & S3). Utilizing a normalization factor and clustering, we compared the miRNA expression profiles of BCSCs and MCF-7 cells, identifying 25 differentially expressed miRNAs that fell into two groups (fold change ≥ 4) using a p-value cutoff of less than 0.05.",0
18,15,"The study conducted real-time RT-PCR for 10 miRNAs, including miR122a, miR-188, miR-200a, miR-21, miR-224, miR-296, miR-301, miR-31, miR-373* and miR-200C. MiR-200C served as the negative control, but it showed no notable difference. The experiments were conducted three times, and eight of the ten tested miRNAs showed concordant results with the microarray data. The electrophoretogram also showed evident and precise bands for the real-time PCR reactions. The Q-RT-PCR results were expressed as the relative ratio between the Q-RT-PCR results for BCSCs and MCF-7 cells, which confirmed the miRNA expression profiles of BCSCs.",0
18,16,"The miRNAs were analyzed through various online software programs, including chromosome localization, sequence analysis, and target prediction. Potential miRNAs related to tumorigenesis and their targets were identified, many of which were oncogenes, anti-oncogenes, or regulatory genes involved in various biological functions such as miRNA processing, transcriptional regulation, signal transduction, apoptosis regulation, and stem cell function. For instance, one of the miRNAs, miR122a, had 161 potential targets consisting of oncogenes, transcription factors, DNA repair-related genes, miRNA processing-related genes, and signal transduction-related genes such as RAD21, G3BP2, CDC42BPB, SP2, GPR172B, GPR172A, MAP3K3, DR1, KHDRBS1, MAP3K12, CCNG1, and DICER1. Another miRNA, miR-21, located on chromosome 17, had 175 potential targets, including pleiomorphic adenoma genes, transcription factors, oncogenes, anti-oncogenes, and genes related to miRNA processing and signal transduction, such as PLAG1, PDCD4, SKI, BCL2, STAT3, PITX2, HBP1, ELF2, E2F3, SPRY1, CDC25A, N-PAC, EIF1AX, EIF2C2, RAB11A, RAB6A, RAB6C, RASGRP1, RHOB, RASA1, TPM1, TGFBI, and TNFSF6. These potential targets existed exclusively in humans, mice, dogs, chimps, and chickens (Additional file 1 table S4).",0
18,25,"We conducted a study to examine the miRNA expression profile of ESA + CD44 + CD24 -/Low BCSCs taken from the MCF-7 cell line. The discovery of miRNAs associated with BCSCs can serve as a basis for further exploration of their functions, providing new insights into the complex nature of BCSCs. It may also aid cancer researchers and clinical oncologists in developing and testing innovative therapeutic methods.",0
